{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Converting .ipynb file into pdf"
      ],
      "metadata": {
        "id": "jeDnM6XE8RFg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysirdkKwzvBt"
      },
      "source": [
        "### Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toCvWcSpt-Mk"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "#mount google drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "#update the system path so that Python knows to look in this folder for libraries\n",
        "sys.path.append('/content/drive/My Drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing latex libraries and converting to pdf"
      ],
      "metadata": {
        "id": "mKGPutjy_3SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd My\\Drive/Colab\\ Notebooks"
      ],
      "metadata": {
        "id": "l5WTluoK8nMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended"
      ],
      "metadata": {
        "id": "-JZ8VR5L8KJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to pdf 'Advanced NLE Assignment Language Models.ipynb'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSyavIjATpdm",
        "outputId": "0f40b506-2085-42c2-a1cd-a565964055cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook Advanced NLE Assignment Language Models.ipynb to pdf\n",
            "[NbConvertApp] Support files will be in Advanced NLE Assignment Language Models_files/\n",
            "[NbConvertApp] Making directory ./Advanced NLE Assignment Language Models_files\n",
            "[NbConvertApp] Making directory ./Advanced NLE Assignment Language Models_files\n",
            "[NbConvertApp] Writing 376252 bytes to ./notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', './notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', './notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 282890 bytes to Advanced NLE Assignment Language Models.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwLcwAc0swEE"
      },
      "source": [
        "# Useful packages & Set Training/Testing Files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtD2ngNGzzX1"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TJuwnAbsrqB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd, csv\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import operator\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#For LSTM model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "#For Word Similarity in Distributional smoothing\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#For word tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize as tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW0D0dgpz4g_"
      },
      "source": [
        "### Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quxw35u-wjy9"
      },
      "outputs": [],
      "source": [
        "#Get all of the training files from Homes_Training data for sentence completion\n",
        "def get_training_files(training_dir,split=1):\n",
        "  filenames=os.listdir(training_dir)\n",
        "  print(\"There are {} files in the training directory: {}\".format(len(filenames),training_dir))\n",
        "  random.shuffle(filenames)\n",
        "  return filenames\n",
        "\n",
        "parentdir=\"/content/drive/My Drive/lab2resources/lab2resources/sentence-completion\"  #'C:/Users/benat/Documents/sentence-completion'   Please enter your training data path\n",
        "trainingdir=os.path.join(parentdir,\"Holmes_Training_Data\")\n",
        "\n",
        "training = get_training_files(trainingdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuDSqTxOuggc"
      },
      "source": [
        "# Visualise the challenge & Create Sentence Completion Challenge Reader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6qI12tBduj5s",
        "outputId": "27f1aa20-d390-43f5-a5df-7fbba60207d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  id  \\\n",
              "0  1   \n",
              "1  2   \n",
              "2  3   \n",
              "3  4   \n",
              "4  5   \n",
              "\n",
              "                                                                                                               question  \\\n",
              "0              I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.   \n",
              "1  It was furnished partly as a sitting and partly as a bedroom , with flowers arranged _____ in every nook and corner.   \n",
              "2               As I descended , my old ally , the _____ , came out of the room and closed the door tightly behind him.   \n",
              "3                                    We got off , _____ our fare , and the trap rattled back on its way to Leatherhead.   \n",
              "4                                     He held in his hand a _____ of blue paper , scrawled over with notes and figures.   \n",
              "\n",
              "         a)               b)             c)         d)             e) answers  \n",
              "0    crying  instantaneously       residing    matched        walking  [1, c]  \n",
              "1  daintily        privately  inadvertently  miserably    comfortably  [2, a]  \n",
              "2      gods             moon        panther      guard  country-dance  [3, d]  \n",
              "3   rubbing         doubling           paid     naming       carrying  [4, c]  \n",
              "4    supply           parcel           sign      sheet         chorus  [5, d]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9a69add-876e-4afb-9516-87e2c899f440\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>a)</th>\n",
              "      <th>b)</th>\n",
              "      <th>c)</th>\n",
              "      <th>d)</th>\n",
              "      <th>e)</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.</td>\n",
              "      <td>crying</td>\n",
              "      <td>instantaneously</td>\n",
              "      <td>residing</td>\n",
              "      <td>matched</td>\n",
              "      <td>walking</td>\n",
              "      <td>[1, c]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>It was furnished partly as a sitting and partly as a bedroom , with flowers arranged _____ in every nook and corner.</td>\n",
              "      <td>daintily</td>\n",
              "      <td>privately</td>\n",
              "      <td>inadvertently</td>\n",
              "      <td>miserably</td>\n",
              "      <td>comfortably</td>\n",
              "      <td>[2, a]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>As I descended , my old ally , the _____ , came out of the room and closed the door tightly behind him.</td>\n",
              "      <td>gods</td>\n",
              "      <td>moon</td>\n",
              "      <td>panther</td>\n",
              "      <td>guard</td>\n",
              "      <td>country-dance</td>\n",
              "      <td>[3, d]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>We got off , _____ our fare , and the trap rattled back on its way to Leatherhead.</td>\n",
              "      <td>rubbing</td>\n",
              "      <td>doubling</td>\n",
              "      <td>paid</td>\n",
              "      <td>naming</td>\n",
              "      <td>carrying</td>\n",
              "      <td>[4, c]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>He held in his hand a _____ of blue paper , scrawled over with notes and figures.</td>\n",
              "      <td>supply</td>\n",
              "      <td>parcel</td>\n",
              "      <td>sign</td>\n",
              "      <td>sheet</td>\n",
              "      <td>chorus</td>\n",
              "      <td>[5, d]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9a69add-876e-4afb-9516-87e2c899f440')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a9a69add-876e-4afb-9516-87e2c899f440 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a9a69add-876e-4afb-9516-87e2c899f440');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "pd.options.display.max_colwidth = 500\n",
        "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
        "answers=os.path.join(parentdir,\"test_answer.csv\")\n",
        "\n",
        "#Visualise the sentences and their possibilities \n",
        "with open(questions) as instream:\n",
        "    csvreader=csv.reader(instream)\n",
        "    lines=list(csvreader)\n",
        "qs_df=pd.DataFrame(lines[1:],columns=lines[0])\n",
        "\n",
        "#Visualise the answers to complete the sentence\n",
        "with open(answers) as instream:\n",
        "    csvreader=csv.reader(instream)\n",
        "    lines=list(csvreader)\n",
        "qs_df[\"answers\"] = lines[1:]\n",
        "qs_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynpbgXr80SO-"
      },
      "outputs": [],
      "source": [
        "class question:\n",
        "\n",
        "    def __init__(self,aline):\n",
        "      #Process sentence\n",
        "      self.fields=aline\n",
        "\n",
        "    def get_tokens(self):\n",
        "      #Seperate sentence into tokens\n",
        "      return [\"__START\"]+tokenize(self.fields[question.colnames[\"question\"]])+[\"__END\"]\n",
        "\n",
        "    def get_field(self,field):\n",
        "      #Get the the field from the possible words to complete the sentence\n",
        "      #Can be a), b), c), d) or e)\n",
        "      return self.fields[question.colnames[field]]\n",
        "    \n",
        "    def add_answer(self,fields):\n",
        "      #Get correct option\n",
        "      self.answer=fields[1]\n",
        "   \n",
        "    def chooseA(self):\n",
        "      #Always choose first possible word\n",
        "      return(\"a\")\n",
        "\n",
        "    def chooseunigram(self,lm):\n",
        "      #Method which uses the unigram language model\n",
        "      choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "      probs=[lm.unigram.get(self.get_field(ch+\")\"),0) for ch in choices]  #Get probability from dictionary (unigram) value from each possible word\n",
        "      maxprob=max(probs)\n",
        "      bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "      return np.random.choice(bestchoices)  #If multiple best options, choose randomly\n",
        "\n",
        "    def get_left_context(self,window=1,target=\"_____\"):\n",
        "      #Get the left context from the target word\n",
        "      found=-1\n",
        "      sent_tokens=self.get_tokens()\n",
        "      for i,token in enumerate(sent_tokens):\n",
        "          if token==target:\n",
        "              found=i\n",
        "              break          \n",
        "      if found>-1:\n",
        "          return sent_tokens[i-window:i]\n",
        "      else:\n",
        "          return []\n",
        "        \n",
        "    def choosebigram(self,lm,method=\"bigram\",choices=[]):\n",
        "      #Method which uses the bigram language model\n",
        "      if choices==[]:\n",
        "          choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "      context=self.get_left_context(window=1)\n",
        "      probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method}) for ch in choices]  #Get probability from language model which uses Discount smoothing\n",
        "      maxprob=max(probs)\n",
        "      bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "      return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "      #Get the predicted word option\n",
        "      if method==\"chooseA\":\n",
        "          return self.chooseA()\n",
        "      elif method==\"random\":\n",
        "          return self.chooserandom()\n",
        "      elif method==\"unigram\":\n",
        "          return self.chooseunigram(lm=lm)\n",
        "      elif method==\"bigram\":\n",
        "          return self.choosebigram(lm=lm)\n",
        "        \n",
        "    def predict_and_score(self,method,lm):\n",
        "      #compare prediction according to method with the correct answer\n",
        "      #return 1 or 0 accordingly\n",
        "      prediction=self.predict(method=method,lm=lm)\n",
        "      if prediction ==self.answer:\n",
        "          return 1\n",
        "      else:\n",
        "          return 0\n",
        "\n",
        "class scc_reader:\n",
        "    \n",
        "    def __init__(self,qs=questions,ans=answers):\n",
        "        self.qs=qs\n",
        "        self.ans=ans\n",
        "        self.read_files()\n",
        "        \n",
        "    def read_files(self):\n",
        "        #read in the question file\n",
        "        with open(self.qs) as instream:\n",
        "            csvreader=csv.reader(instream)\n",
        "            qlines=list(csvreader)\n",
        "        \n",
        "        #store the column names as a reverse index so they can be used to reference parts of the question\n",
        "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
        "        \n",
        "        #create a question instance for each line of the file (other than heading line)\n",
        "        self.questions=[question(qline) for qline in qlines[1:]]\n",
        "        \n",
        "        #read in the answer file\n",
        "        with open(self.ans) as instream:\n",
        "            csvreader=csv.reader(instream)\n",
        "            alines=list(csvreader)\n",
        "            \n",
        "        #add answers to questions so predictions can be checked    \n",
        "        for q,aline in zip(self.questions,alines[1:]):\n",
        "            q.add_answer(aline)\n",
        "        \n",
        "    def get_field(self,field):\n",
        "      #Get field a), b), c) or d) from word possibilities\n",
        "      return [q.get_field(field) for q in self.questions] \n",
        "    \n",
        "    def predict(self,method,lm):\n",
        "      #Get predicted word option\n",
        "      return [q.predict(method=method,lm=lm) for q in self.questions]\n",
        "    \n",
        "    def predict_and_score(self,method,lm):\n",
        "        #Used to predict model accuracy\n",
        "        scores=[q.predict_and_score(method=method,lm=lm) for q in self.questions]\n",
        "        return sum(scores)/len(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnqoxwsM0fPG",
        "outputId": "21618dfa-669c-487f-d11c-677c4c6a8087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['crying', 'daintily', 'gods', 'rubbing']\n",
            "['a', 'a', 'a', 'a']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19903846153846153"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#Check if the reader is working\n",
        "SCC = scc_reader()\n",
        "print(SCC.get_field(\"a)\")[:4])\n",
        "print(SCC.predict(method=\"chooseA\",lm=None)[:4])\n",
        "SCC.predict_and_score(method=\"chooseA\",lm=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqekk65dy-X2"
      },
      "source": [
        "# Language models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_njUhkgzMCE"
      },
      "outputs": [],
      "source": [
        "class language_model():\n",
        "    def __init__(self,trainingdir=parentdir,files=[]):\n",
        "        self.training_dir=trainingdir\n",
        "        self.files=files\n",
        "        self.train()    \n",
        "        \n",
        "    def train(self):\n",
        "        self.unigram={}\n",
        "        self.bigram={}\n",
        "\n",
        "        self.start = False  #Starting point of when to get tokens from training data\n",
        "        self.end_tags = [\"End of Project Gutenberg's Etext\", \"End of Project Gutenberg Etext\", \"End of the Project Gutenberg Etext\", \"End of the Project Gutenberg eText\"]  #End mark for metadata\n",
        "        self._processfiles()\n",
        "\n",
        "        self._make_unknowns()\n",
        "        self._discount()\n",
        "        self._kneser_ney()\n",
        "\n",
        "        self._convert_to_probs()\n",
        "        \n",
        "    \n",
        "    def _processline(self,line):\n",
        "      #Process each line in the file\n",
        "      #Only process sentence if sentence is not in the metadata\n",
        "      if \"end of project gutenberg\" not in line.lower() and \"end of the project gutenberg\" not in line.lower():\n",
        "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"] #Add Start and End padding to tokenized sentence\n",
        "        previous=\"__END\"\n",
        "        for token in tokens:\n",
        "            self.unigram[token]=self.unigram.get(token,0)+1\n",
        "            current=self.bigram.get(previous,{})\n",
        "            current[token]=current.get(token,0)+1\n",
        "            self.bigram[previous]=current\n",
        "            previous=token\n",
        "            \n",
        "    \n",
        "    def _processfiles(self, remove_metadata=True):\n",
        "      if remove_metadata:\n",
        "        #Process each file\n",
        "        for afile in self.files:\n",
        "          print(\"Processing {}\".format(afile))\n",
        "          try:\n",
        "              with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "                  for line in instream:\n",
        "                      line=line.rstrip()\n",
        "                      if len(line)>0 and self.start:\n",
        "                          self._processline(line)\n",
        "                      if \"*END*\" in line:\n",
        "                        self.start = True\n",
        "                  self.start = False\n",
        "          except UnicodeDecodeError:\n",
        "              print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
        "      else:\n",
        "        for file in training[:1]:\n",
        "          print(\"Processing {}\".format(file))\n",
        "          try: \n",
        "            with open(os.path.join(trainingdir,file)) as instream:\n",
        "              for line in instream:\n",
        "                line=line.rstrip()\n",
        "                if len(line)>0:\n",
        "                  tokens=[\"__START\"]+tokenize(line)+[\"__END\"] #Add Start and End padding to tokenized sentence\n",
        "                  previous=\"__END\"\n",
        "                  for token in tokens:\n",
        "                    self.unigram[token]=self.unigram.get(token,0)+1\n",
        "                    current=self.bigram.get(previous,{})\n",
        "                    current[token]=current.get(token,0)+1\n",
        "                    self.bigram[previous]=current\n",
        "                    previous=token\n",
        "          except UnicodeDecodeError:\n",
        "            print(\"UnicodeDecodeError processing {}: ignoring file\".format(file))\n",
        "\n",
        "      \n",
        "            \n",
        "    def _convert_to_probs(self):\n",
        "      #To get the probabilities, divide frequency of a certain word and divided by all occurring words\n",
        "      self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
        "      self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
        "      self.kn={k:v/sum(self.kn.values()) for (k,v) in self.kn.items()}\n",
        "        \n",
        "      ###adjust __UNK probabilities to include probability of an individual unknown word (1/number_unknowns) \n",
        "      self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)/self.number_unknowns\n",
        "      self.bigram[\"__UNK\"]={k:v/self.number_unknowns for (k,v) in self.bigram.get(\"__UNK\",{}).items()}\n",
        "      for key,adict in self.bigram.items():\n",
        "          adict[\"__UNK\"]=adict.get(\"__UNK\",0)/self.number_unknowns\n",
        "          self.bigram[key]=adict\n",
        "      self.kn[\"__UNK\"]=self.kn.get(\"__UNK\",0)/self.number_unknowns\n",
        "    \n",
        "        \n",
        "    def get_prob(self,token,context=\"\",methodparams={}):\n",
        "      #Get probability from unigram/bigram, plus add some smoothing\n",
        "      if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
        "          return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "      else:\n",
        "          if methodparams.get(\"smoothing\",\"kneser-ney\")==\"kneser-ney\":\n",
        "              unidist=self.kn\n",
        "          else:\n",
        "              unidist=self.unigram\n",
        "          bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
        "          big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
        "          lmbda=bigram[\"__DISCOUNT\"]\n",
        "          uni_p=unidist.get(token,unidist.get(\"__UNK\",0))\n",
        "          p=big_p+lmbda*uni_p            \n",
        "          return p\n",
        "    \n",
        "    \n",
        "    def nextlikely(self,k=1,current=\"\",method=\"unigram\"):\n",
        "        #use probabilities according to method to generate a likely next sequence\n",
        "        #choose random token from k best\n",
        "        blacklist=[\"__START\",\"__UNK\",\"__DISCOUNT\"]\n",
        "       \n",
        "        if method==\"unigram\":\n",
        "            dist=self.unigram\n",
        "        else:\n",
        "            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))\n",
        "    \n",
        "        #sort the tokens by unigram probability\n",
        "        mostlikely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
        "        #filter out any undesirable tokens\n",
        "        filtered=[w for (w,p) in mostlikely if w not in blacklist]\n",
        "        #choose one randomly from the top k\n",
        "        res=random.choice(filtered[:k])\n",
        "        return res\n",
        "    \n",
        "    def generate(self,k=1,end=\"__END\",limit=20,method=\"bigram\",methodparams={}):\n",
        "      #Generate a sentence\n",
        "        if method==\"\":\n",
        "            method=methodparams.get(\"method\",\"bigram\")\n",
        "        current=\"__START\"\n",
        "        tokens=[]\n",
        "        while current!=end and len(tokens)<limit:\n",
        "            current=self.nextlikely(k=k,current=current,method=method)\n",
        "            tokens.append(current)\n",
        "        return \" \".join(tokens[:-1])\n",
        "    \n",
        "    \n",
        "    def compute_prob_line(self,line,methodparams={}):\n",
        "        #this will add _start to the beginning of a line of text\n",
        "        #compute the probability of the line according to the desired model\n",
        "        #and returns probability together with number of tokens\n",
        "        \n",
        "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
        "        acc=0\n",
        "        for i,token in enumerate(tokens[1:]):\n",
        "            acc+=math.log(self.get_prob(token,tokens[:i+1],methodparams))\n",
        "        return acc,len(tokens[1:])\n",
        "    \n",
        "    def compute_probability(self,filenames=[],methodparams={}):\n",
        "        #computes the probability (and length) of a corpus contained in filenames\n",
        "        if filenames==[]:\n",
        "            filenames=self.files\n",
        "        \n",
        "        total_p=0\n",
        "        total_N=0\n",
        "        for i,afile in enumerate(filenames):\n",
        "            print(\"Processing file {}:{}\".format(i,afile))\n",
        "            try:\n",
        "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "                    for line in instream:\n",
        "                        line=line.rstrip()\n",
        "                        if len(line)>0:\n",
        "                            p,N=self.compute_prob_line(line,methodparams=methodparams)\n",
        "                            total_p+=p\n",
        "                            total_N+=N\n",
        "            except UnicodeDecodeError:\n",
        "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
        "        return total_p,total_N\n",
        "    \n",
        "    def compute_perplexity(self,filenames=[],methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n",
        "        \n",
        "        #compute the probability and length of the corpus\n",
        "        #calculate perplexity\n",
        "        #lower perplexity means that the model better explains the data\n",
        "        \n",
        "        p,N=self.compute_probability(filenames=filenames,methodparams=methodparams)\n",
        "        #print(p,N)\n",
        "        pp=math.exp(-p/N)\n",
        "        return pp  \n",
        "    \n",
        "    def _make_unknowns(self,known=100):\n",
        "      #Convert least occuring words in the language models into Out-Of-Vocabulary \"__UNK\" tokens\n",
        "        unknown=0\n",
        "        self.number_unknowns=0\n",
        "        for (k,v) in list(self.unigram.items()):  #In unigrams do for each key\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "                self.number_unknowns+=1\n",
        "        for (k,adict) in list(self.bigram.items()): #In bigrams do for each key and nested key\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "                \n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "                \n",
        "    def _discount(self,discount=0.75):\n",
        "        #discount each bigram count by a small fixed amount\n",
        "        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n",
        "        \n",
        "        #for each word, store the total amount of the discount so that the total is the same \n",
        "        #i.e., so we are reserving this as probability mass\n",
        "        for k in self.bigram.keys():\n",
        "            lamb=len(self.bigram[k])\n",
        "            self.bigram[k][\"__DISCOUNT\"]=lamb*discount\n",
        "               \n",
        "    def _kneser_ney(self):\n",
        "        #work out kneser-ney unigram probabilities\n",
        "        #count the number of contexts each word has been seen in\n",
        "        self.kn={}\n",
        "        for (k,adict) in self.bigram.items():\n",
        "            for kk in adict.keys():\n",
        "                self.kn[kk]=self.kn.get(kk,0)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gM13Op2PcAv"
      },
      "outputs": [],
      "source": [
        "lm = language_model(trainingdir=trainingdir,files=training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2azPmKUzHtF"
      },
      "source": [
        "### Unigram & Bigram models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFX-_Pl02gts"
      },
      "source": [
        "###Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Js49g8Z_yfd"
      },
      "outputs": [],
      "source": [
        "def get_left_context(sent_tokens,window,target=\"_____\"):\n",
        "    found=-1\n",
        "    for i,token in enumerate(sent_tokens):\n",
        "        if token==target:\n",
        "            found=i\n",
        "            break \n",
        "            \n",
        "    if found>-1:\n",
        "        return sent_tokens[i-window:i]\n",
        "    else:\n",
        "        return []\n",
        "    \n",
        "\n",
        "qs_df['tokens']=qs_df['question'].map(tokenize)\n",
        "qs_df['left_context']=qs_df['tokens'].map(lambda x: get_left_context(x,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "p6DR9TAG2ijk",
        "outputId": "a95234b5-d5b3-42a9-d3f5-2994a4306aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for unigram 0.20798076923076922\n",
            "Score for bigram 0.2021153846153846\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  id  \\\n",
              "0  1   \n",
              "1  2   \n",
              "2  3   \n",
              "3  4   \n",
              "4  5   \n",
              "\n",
              "                                                                                                               question  \\\n",
              "0              I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.   \n",
              "1  It was furnished partly as a sitting and partly as a bedroom , with flowers arranged _____ in every nook and corner.   \n",
              "2               As I descended , my old ally , the _____ , came out of the room and closed the door tightly behind him.   \n",
              "3                                    We got off , _____ our fare , and the trap rattled back on its way to Leatherhead.   \n",
              "4                                     He held in his hand a _____ of blue paper , scrawled over with notes and figures.   \n",
              "\n",
              "         a)               b)             c)         d)             e) answers  \\\n",
              "0    crying  instantaneously       residing    matched        walking  [1, c]   \n",
              "1  daintily        privately  inadvertently  miserably    comfortably  [2, a]   \n",
              "2      gods             moon        panther      guard  country-dance  [3, d]   \n",
              "3   rubbing         doubling           paid     naming       carrying  [4, c]   \n",
              "4    supply           parcel           sign      sheet         chorus  [5, d]   \n",
              "\n",
              "                                                                                                                                          tokens  \\\n",
              "0              [I, have, it, from, the, same, source, that, you, are, both, an, orphan, and, a, bachelor, and, are, _____, alone, in, London, .]   \n",
              "1  [It, was, furnished, partly, as, a, sitting, and, partly, as, a, bedroom, ,, with, flowers, arranged, _____, in, every, nook, and, corner, .]   \n",
              "2              [As, I, descended, ,, my, old, ally, ,, the, _____, ,, came, out, of, the, room, and, closed, the, door, tightly, behind, him, .]   \n",
              "3                                        [We, got, off, ,, _____, our, fare, ,, and, the, trap, rattled, back, on, its, way, to, Leatherhead, .]   \n",
              "4                                          [He, held, in, his, hand, a, _____, of, blue, paper, ,, scrawled, over, with, notes, and, figures, .]   \n",
              "\n",
              "          left_context bigram_pred unigram_pred  \n",
              "0           [and, are]           a            c  \n",
              "1  [flowers, arranged]           c            d  \n",
              "2             [,, the]           e            d  \n",
              "3             [off, ,]           b            c  \n",
              "4            [hand, a]           e            a  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3326be6f-62fa-4b58-bd8c-170b8dbcf525\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>a)</th>\n",
              "      <th>b)</th>\n",
              "      <th>c)</th>\n",
              "      <th>d)</th>\n",
              "      <th>e)</th>\n",
              "      <th>answers</th>\n",
              "      <th>tokens</th>\n",
              "      <th>left_context</th>\n",
              "      <th>bigram_pred</th>\n",
              "      <th>unigram_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.</td>\n",
              "      <td>crying</td>\n",
              "      <td>instantaneously</td>\n",
              "      <td>residing</td>\n",
              "      <td>matched</td>\n",
              "      <td>walking</td>\n",
              "      <td>[1, c]</td>\n",
              "      <td>[I, have, it, from, the, same, source, that, you, are, both, an, orphan, and, a, bachelor, and, are, _____, alone, in, London, .]</td>\n",
              "      <td>[and, are]</td>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>It was furnished partly as a sitting and partly as a bedroom , with flowers arranged _____ in every nook and corner.</td>\n",
              "      <td>daintily</td>\n",
              "      <td>privately</td>\n",
              "      <td>inadvertently</td>\n",
              "      <td>miserably</td>\n",
              "      <td>comfortably</td>\n",
              "      <td>[2, a]</td>\n",
              "      <td>[It, was, furnished, partly, as, a, sitting, and, partly, as, a, bedroom, ,, with, flowers, arranged, _____, in, every, nook, and, corner, .]</td>\n",
              "      <td>[flowers, arranged]</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>As I descended , my old ally , the _____ , came out of the room and closed the door tightly behind him.</td>\n",
              "      <td>gods</td>\n",
              "      <td>moon</td>\n",
              "      <td>panther</td>\n",
              "      <td>guard</td>\n",
              "      <td>country-dance</td>\n",
              "      <td>[3, d]</td>\n",
              "      <td>[As, I, descended, ,, my, old, ally, ,, the, _____, ,, came, out, of, the, room, and, closed, the, door, tightly, behind, him, .]</td>\n",
              "      <td>[,, the]</td>\n",
              "      <td>e</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>We got off , _____ our fare , and the trap rattled back on its way to Leatherhead.</td>\n",
              "      <td>rubbing</td>\n",
              "      <td>doubling</td>\n",
              "      <td>paid</td>\n",
              "      <td>naming</td>\n",
              "      <td>carrying</td>\n",
              "      <td>[4, c]</td>\n",
              "      <td>[We, got, off, ,, _____, our, fare, ,, and, the, trap, rattled, back, on, its, way, to, Leatherhead, .]</td>\n",
              "      <td>[off, ,]</td>\n",
              "      <td>b</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>He held in his hand a _____ of blue paper , scrawled over with notes and figures.</td>\n",
              "      <td>supply</td>\n",
              "      <td>parcel</td>\n",
              "      <td>sign</td>\n",
              "      <td>sheet</td>\n",
              "      <td>chorus</td>\n",
              "      <td>[5, d]</td>\n",
              "      <td>[He, held, in, his, hand, a, _____, of, blue, paper, ,, scrawled, over, with, notes, and, figures, .]</td>\n",
              "      <td>[hand, a]</td>\n",
              "      <td>e</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3326be6f-62fa-4b58-bd8c-170b8dbcf525')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3326be6f-62fa-4b58-bd8c-170b8dbcf525 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3326be6f-62fa-4b58-bd8c-170b8dbcf525');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#Check if it is working\n",
        "SCC_unigram = scc_reader()\n",
        "SCC_bigram = scc_reader()\n",
        "\n",
        "uni_average = []\n",
        "bi_average = []\n",
        "for i in range(10):\n",
        "  acc = SCC_unigram.predict_and_score(method=\"unigram\",lm=lm)\n",
        "  uni_average.append(acc)\n",
        "\n",
        "  acc = SCC_bigram.predict_and_score(method=\"bigram\",lm=lm)\n",
        "  bi_average.append(acc)  \n",
        "\n",
        "print(\"Score for unigram\",sum(uni_average)/len(uni_average))\n",
        "print(\"Score for bigram\",sum(bi_average)/len(bi_average))\n",
        "\n",
        "qs_df[\"bigram_pred\"]=SCC.predict(method=\"bigram\",lm=lm)\n",
        "qs_df[\"unigram_pred\"]=SCC.predict(method=\"unigram\",lm=lm)\n",
        "qs_df[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Metadata probability"
      ],
      "metadata": {
        "id": "0ei7fseU_3Fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training without metadata"
      ],
      "metadata": {
        "id": "RyyjqvydA1tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sort_orders = sorted(lm.bigram.get(\"public\").items(), key=lambda x: x[1], reverse=True)\n",
        "sort_orders[:3]"
      ],
      "metadata": {
        "id": "y64MyLlxATDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6f3ea9-e9e4-4580-a3ab-6ab324160601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('__DISCOUNT', 0.22330097087378642),\n",
              " ('__END', 0.08940129449838188),\n",
              " (',', 0.06351132686084142)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with metadata"
      ],
      "metadata": {
        "id": "SnvPa8xzA5dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class language_model():\n",
        "    def __init__(self,trainingdir=parentdir,files=[]):\n",
        "        self.training_dir=trainingdir\n",
        "        self.files=files\n",
        "        self.train()    \n",
        "        \n",
        "    def train(self):\n",
        "        self.unigram={}\n",
        "        self.bigram={}\n",
        "\n",
        "        self.start = False  #Starting point of when to get tokens from training data\n",
        "        self.end_tags = [\"End of Project Gutenberg's Etext\", \"End of Project Gutenberg Etext\", \"End of the Project Gutenberg Etext\", \"End of the Project Gutenberg eText\"]  #End mark for metadata\n",
        "        self._processfiles()\n",
        "\n",
        "        self._make_unknowns()\n",
        "        self._discount()\n",
        "        self._kneser_ney()\n",
        "\n",
        "        self._convert_to_probs()\n",
        "        \n",
        "    \n",
        "    def _processline(self,line):\n",
        "      #Process each line in the file\n",
        "      #Only process sentence if sentence is not in the metadata\n",
        "      if \"end of project gutenberg\" not in line.lower() and \"end of the project gutenberg\" not in line.lower():\n",
        "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"] #Add Start and End padding to tokenized sentence\n",
        "        previous=\"__END\"\n",
        "        for token in tokens:\n",
        "            self.unigram[token]=self.unigram.get(token,0)+1\n",
        "            current=self.bigram.get(previous,{})\n",
        "            current[token]=current.get(token,0)+1\n",
        "            self.bigram[previous]=current\n",
        "            previous=token\n",
        "            \n",
        "    \n",
        "    def _processfiles(self, remove_metadata=False):\n",
        "      if remove_metadata:\n",
        "        #Process each file\n",
        "        for afile in self.files:\n",
        "          print(\"Processing {}\".format(afile))\n",
        "          try:\n",
        "              with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "                  for line in instream:\n",
        "                      line=line.rstrip()\n",
        "                      if len(line)>0 and self.start:\n",
        "                          self._processline(line)\n",
        "                      if \"*END*\" in line:\n",
        "                        self.start = True\n",
        "                  self.start = False\n",
        "          except UnicodeDecodeError:\n",
        "              print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
        "      else:\n",
        "        for afile in self.files:\n",
        "          print(\"Processing {}\".format(afile))\n",
        "          try: \n",
        "            with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "              for line in instream:\n",
        "                line=line.rstrip()\n",
        "                if len(line)>0:\n",
        "                  tokens=[\"__START\"]+tokenize(line)+[\"__END\"] #Add Start and End padding to tokenized sentence\n",
        "                  previous=\"__END\"\n",
        "                  for token in tokens:\n",
        "                    self.unigram[token]=self.unigram.get(token,0)+1\n",
        "                    current=self.bigram.get(previous,{})\n",
        "                    current[token]=current.get(token,0)+1\n",
        "                    self.bigram[previous]=current\n",
        "                    previous=token\n",
        "          except UnicodeDecodeError:\n",
        "            print(\"UnicodeDecodeError processing {}: ignoring file\".format(file))\n",
        "\n",
        "      \n",
        "            \n",
        "    def _convert_to_probs(self):\n",
        "      #To get the probabilities, divide frequency of a certain word and divided by all occurring words\n",
        "      self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
        "      self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
        "      self.kn={k:v/sum(self.kn.values()) for (k,v) in self.kn.items()}\n",
        "        \n",
        "      ###adjust __UNK probabilities to include probability of an individual unknown word (1/number_unknowns) \n",
        "      self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)/self.number_unknowns\n",
        "      self.bigram[\"__UNK\"]={k:v/self.number_unknowns for (k,v) in self.bigram.get(\"__UNK\",{}).items()}\n",
        "      for key,adict in self.bigram.items():\n",
        "          adict[\"__UNK\"]=adict.get(\"__UNK\",0)/self.number_unknowns\n",
        "          self.bigram[key]=adict\n",
        "      self.kn[\"__UNK\"]=self.kn.get(\"__UNK\",0)/self.number_unknowns\n",
        "    \n",
        "        \n",
        "    def get_prob(self,token,context=\"\",methodparams={}):\n",
        "      #Get probability from unigram/bigram, plus add some smoothing\n",
        "      if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
        "          return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "      else:\n",
        "          if methodparams.get(\"smoothing\",\"kneser-ney\")==\"kneser-ney\":\n",
        "              unidist=self.kn\n",
        "          else:\n",
        "              unidist=self.unigram\n",
        "          bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
        "          big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
        "          lmbda=bigram[\"__DISCOUNT\"]\n",
        "          uni_p=unidist.get(token,unidist.get(\"__UNK\",0))\n",
        "          p=big_p+lmbda*uni_p            \n",
        "          return p\n",
        "    \n",
        "    \n",
        "    def nextlikely(self,k=10,current=\"\",method=\"unigram\"):\n",
        "        #use probabilities according to method to generate a likely next sequence\n",
        "        #choose random token from k best\n",
        "        blacklist=[\"__START\",\"__UNK\",\"__DISCOUNT\"]\n",
        "       \n",
        "        if method==\"unigram\":\n",
        "            dist=self.unigram\n",
        "        else:\n",
        "            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))\n",
        "    \n",
        "        #sort the tokens by unigram probability\n",
        "        mostlikely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
        "        #filter out any undesirable tokens\n",
        "        filtered=[w for (w,p) in mostlikely if w not in blacklist]\n",
        "        #choose one randomly from the top k\n",
        "        res=random.choice(filtered[:k])\n",
        "        return res\n",
        "    \n",
        "    def generate(self,k=1,end=\"__END\",limit=20,method=\"bigram\",methodparams={}):\n",
        "      #Generate a sentence\n",
        "        if method==\"\":\n",
        "            method=methodparams.get(\"method\",\"bigram\")\n",
        "        current=\"__START\"\n",
        "        tokens=[]\n",
        "        while current!=end and len(tokens)<limit:\n",
        "            current=self.nextlikely(k=k,current=current,method=method)\n",
        "            tokens.append(current)\n",
        "        return \" \".join(tokens[:-1])\n",
        "    \n",
        "    \n",
        "    def compute_prob_line(self,line,methodparams={}):\n",
        "        #this will add _start to the beginning of a line of text\n",
        "        #compute the probability of the line according to the desired model\n",
        "        #and returns probability together with number of tokens\n",
        "        \n",
        "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
        "        acc=0\n",
        "        for i,token in enumerate(tokens[1:]):\n",
        "            acc+=math.log(self.get_prob(token,tokens[:i+1],methodparams))\n",
        "        return acc,len(tokens[1:])\n",
        "    \n",
        "    def compute_probability(self,filenames=[],methodparams={}):\n",
        "        #computes the probability (and length) of a corpus contained in filenames\n",
        "        if filenames==[]:\n",
        "            filenames=self.files\n",
        "        \n",
        "        total_p=0\n",
        "        total_N=0\n",
        "        for i,afile in enumerate(filenames):\n",
        "            print(\"Processing file {}:{}\".format(i,afile))\n",
        "            try:\n",
        "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "                    for line in instream:\n",
        "                        line=line.rstrip()\n",
        "                        if len(line)>0:\n",
        "                            p,N=self.compute_prob_line(line,methodparams=methodparams)\n",
        "                            total_p+=p\n",
        "                            total_N+=N\n",
        "            except UnicodeDecodeError:\n",
        "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
        "        return total_p,total_N\n",
        "    \n",
        "    def compute_perplexity(self,filenames=[],methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n",
        "        \n",
        "        #compute the probability and length of the corpus\n",
        "        #calculate perplexity\n",
        "        #lower perplexity means that the model better explains the data\n",
        "        \n",
        "        p,N=self.compute_probability(filenames=filenames,methodparams=methodparams)\n",
        "        #print(p,N)\n",
        "        pp=math.exp(-p/N)\n",
        "        return pp  \n",
        "    \n",
        "    def _make_unknowns(self,known=100):\n",
        "      #Convert least occuring words in the language models into Out-Of-Vocabulary \"__UNK\" tokens\n",
        "        unknown=0\n",
        "        self.number_unknowns=0\n",
        "        for (k,v) in list(self.unigram.items()):  #In unigrams do for each key\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "                self.number_unknowns+=1\n",
        "        for (k,adict) in list(self.bigram.items()): #In bigrams do for each key and nested key\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "                \n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "                \n",
        "    def _discount(self,discount=0.75):\n",
        "        #discount each bigram count by a small fixed amount\n",
        "        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n",
        "        \n",
        "        #for each word, store the total amount of the discount so that the total is the same \n",
        "        #i.e., so we are reserving this as probability mass\n",
        "        for k in self.bigram.keys():\n",
        "            lamb=len(self.bigram[k])\n",
        "            self.bigram[k][\"__DISCOUNT\"]=lamb*discount\n",
        "               \n",
        "    def _kneser_ney(self):\n",
        "        #work out kneser-ney unigram probabilities\n",
        "        #count the number of contexts each word has been seen in\n",
        "        self.kn={}\n",
        "        for (k,adict) in self.bigram.items():\n",
        "            for kk in adict.keys():\n",
        "                self.kn[kk]=self.kn.get(kk,0)+1"
      ],
      "metadata": {
        "id": "uSAA6yN5Biwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = language_model(trainingdir=trainingdir,files=training)"
      ],
      "metadata": {
        "id": "1OTr2UqjBs14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sort_orders = sorted(lm.bigram.get(\"public\").items(), key=lambda x: x[1], reverse=True)\n",
        "sort_orders[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px_XEQtwDWO8",
        "outputId": "8a71767a-e60b-4645-c1b8-b03d368054ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('domain', 0.32380174291938996),\n",
              " ('__DISCOUNT', 0.1511437908496732),\n",
              " ('__END', 0.06236383442265795)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXsFr75a3N9n"
      },
      "source": [
        "### Quadrigram with Distributional and Back-Off smoothing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdNIQn4YN7xX"
      },
      "outputs": [],
      "source": [
        "class language_model(language_model):\n",
        "      def train(self):\n",
        "        self.unigram={}\n",
        "        self.bigram={}\n",
        "        self.trigram={}\n",
        "        self.quadrigram={}\n",
        "        self.data = []\n",
        "\n",
        "        self.start = False\n",
        "        self.end_tags = [\"End of Project Gutenberg's Etext\", \"End of Project Gutenberg Etext\", \"End of the Project Gutenberg Etext\", \"End of the Project Gutenberg eText\"]\n",
        "        self._processfiles()\n",
        "\n",
        "        self.wordsim_model = gensim.models.Word2Vec(self.data, min_count = 1, vector_size = 100, window = 5)\n",
        "\n",
        "        self._make_unknowns()\n",
        "\n",
        "        self._convert_to_probs() \n",
        "\n",
        "      def _processline(self,line):\n",
        "        if \"end of project gutenberg\" not in line.lower() and \"end of the project gutenberg\" not in line.lower():\n",
        "          self.data.append(tokenize(line))\n",
        "\n",
        "          tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
        "          previous=\"__END\"\n",
        "          for token in tokens:\n",
        "            self.unigram[token]=self.unigram.get(token,0)+1\n",
        "\n",
        "            current=self.bigram.get(previous,{})\n",
        "            current[token]=current.get(token,0)+1\n",
        "            self.bigram[previous]=current\n",
        "            previous=token\n",
        "\n",
        "          tokens.insert(0,\"__END\")\n",
        "          for i in range(len(tokens)-2):\n",
        "            prev=(tokens[i],tokens[i+1])\n",
        "            current=self.trigram.get(prev,{})\n",
        "            current[tokens[i+2]]=current.get(tokens[i+2],0)+1\n",
        "            self.trigram[prev]=current\n",
        "\n",
        "          for i in range(len(tokens)-3):\n",
        "            prev=(tokens[i],tokens[i+1],tokens[i+2])\n",
        "            current=self.quadrigram.get(prev,{})\n",
        "            current[tokens[i+3]]=current.get(tokens[i+3],0)+1\n",
        "            self.quadrigram[prev]=current\n",
        "\n",
        "      def _make_unknowns(self,known=100):\n",
        "        unknown=0\n",
        "        self.number_unknowns=0\n",
        "        for (k,v) in list(self.unigram.items()):\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "                self.number_unknowns+=1\n",
        "\n",
        "        for (k,adict) in list(self.bigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "\n",
        "        for (k,adict) in list(self.trigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            for word in k:\n",
        "              isknown=self.unigram.get(word,0)\n",
        "              if isknown==0:\n",
        "                del self.trigram[k]\n",
        "                k = list(k)\n",
        "                k[k.index(word)] = \"__UNK\"\n",
        "                k = tuple(k)\n",
        "                current=self.trigram.get(k,{})\n",
        "                current.update(adict)\n",
        "                self.trigram[k]=current\n",
        "              else:\n",
        "                self.trigram[k]=adict \n",
        "\n",
        "        for (k,adict) in list(self.quadrigram.items()):\n",
        "          for(kk,v) in list(adict.items()):\n",
        "            isknown=self.unigram.get(kk,0)\n",
        "            if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "              adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "              del adict[kk]\n",
        "          for word in k:\n",
        "            isknown=self.unigram.get(word,0)\n",
        "            if isknown==0:\n",
        "                del self.quadrigram[k]\n",
        "                k = list(k)\n",
        "                k[k.index(word)] = \"__UNK\"\n",
        "                k = tuple(k)\n",
        "                current=self.quadrigram.get(k,{})\n",
        "                current.update(adict)\n",
        "                self.quadrigram[k]=current\n",
        "            else:\n",
        "                self.quadrigram[k]=adict \n",
        "\n",
        "      def _convert_to_probs(self):\n",
        "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
        "        self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
        "        self.trigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.trigram.items()}\n",
        "        self.quadrigram = {key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.quadrigram.items()}\n",
        "        if self.number_unknowns > 0:\n",
        "          self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)/self.number_unknowns\n",
        "          self.bigram[\"__UNK\"]={k:v/self.number_unknowns for (k,v) in self.bigram.get(\"__UNK\",{}).items()}\n",
        "          for key,adict in self.bigram.items():\n",
        "            adict[\"__UNK\"]=adict.get(\"__UNK\",0)/self.number_unknowns\n",
        "            self.bigram[key]=adict\n",
        "\n",
        "          for key, value in self.trigram.items():\n",
        "            if \"__UNK\" in key:\n",
        "              self.trigram[key]={k:v/self.number_unknowns for (k,v) in self.trigram.get(key,{}).items()}\n",
        "            \n",
        "          for key,adict in self.trigram.items():\n",
        "              adict[\"__UNK\"]=adict.get(\"__UNK\",0)/self.number_unknowns\n",
        "              self.trigram[key]=adict            \n",
        "\n",
        "          for key, value in self.quadrigram.items():\n",
        "            if \"__UNK\" in key:\n",
        "              self.quadrigram[key]={k:v/self.number_unknowns for (k,v) in self.quadrigram.get(key,{}).items()}\n",
        "            \n",
        "          for key,adict in self.quadrigram.items():\n",
        "              adict[\"__UNK\"]=adict.get(\"__UNK\",0)/self.number_unknowns\n",
        "              self.quadrigram[key]=adict\n",
        "\n",
        "      def get_prob(self,token,context,methodparams,smoothing,lmbda):\n",
        "        if smoothing==\"back_off\":\n",
        "          if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
        "            return lmbda*lmbda*lmbda*self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "          else:\n",
        "            if methodparams.get(\"method\",\"bigram\")==\"bigram\":\n",
        "              bigram=self.bigram.get(context[0],self.bigram.get(\"__UNK\",{}))\n",
        "              big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
        "              if big_p == 0:\n",
        "                big_p = self.get_prob(token=token,context=context,methodparams={\"method\":\"unigram\"},smoothing=\"back_off\",lmbda=lmbda)\n",
        "                return big_p\n",
        "              else:\n",
        "                return big_p*lmbda*lmbda\n",
        "\n",
        "            elif methodparams.get(\"method\",\"trigram\")==\"trigram\":\n",
        "              trigram=self.trigram.get(tuple(context),self.trigram.get(\"__UNK\",{}))\n",
        "              big_p=trigram.get(token,trigram.get(\"__UNK\",0))\n",
        "              if big_p == 0:\n",
        "                big_p = self.get_prob(token=token,context=context,methodparams={\"method\":\"bigram\"},smoothing=\"back_off\",lmbda=lmbda)\n",
        "                return big_p\n",
        "              else:\n",
        "                return big_p*lmbda\n",
        "\n",
        "            else:\n",
        "              quadrigram=self.quadrigram.get(tuple(context),self.quadrigram.get(\"__UNK\",{}))\n",
        "              big_p=quadrigram.get(token,quadrigram.get(\"__UNK\",0))\n",
        "              if big_p == 0:\n",
        "                big_p = self.get_prob(token=token,context=context,methodparams={\"method\":\"trigram\"},smoothing=\"back_off\",lmbda=lmbda)\n",
        "              return big_p\n",
        "\n",
        "        elif smoothing==\"distributional\":\n",
        "          if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
        "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "          else:\n",
        "            big_p = 0\n",
        "            if methodparams.get(\"method\",\"bigram\")==\"bigram\":\n",
        "              bigram=self.bigram.get(context[0],self.bigram.get(\"__UNK\",{}))\n",
        "              if token in self.wordsim_model.wv.key_to_index:\n",
        "                similar_words = self.wordsim_model.wv.most_similar(token)\n",
        "                for word,prob in similar_words:\n",
        "                  small_p=bigram.get(word,bigram.get(\"__UNK\",0))*(prob/sum([a_tuple[1] for a_tuple in similar_words]))\n",
        "                  big_p+=small_p\n",
        "              else: \n",
        "                big_p = bigram.get(token,bigram.get(\"__UNK\",0))\n",
        "            elif methodparams.get(\"method\",\"trigram\")==\"trigram\":\n",
        "              trigram=self.trigram.get(tuple(context),self.trigram.get(\"__UNK\",{}))\n",
        "              if token in self.wordsim_model.wv.key_to_index:\n",
        "                similar_words = self.wordsim_model.wv.most_similar(token)\n",
        "                for word,prob in self.wordsim_model.wv.most_similar(token):\n",
        "                  small_p=trigram.get(word,trigram.get(\"__UNK\",0))*(prob/sum([a_tuple[1] for a_tuple in similar_words]))\n",
        "                  big_p+=small_p\n",
        "              else:\n",
        "                big_p = trigram.get(token,trigram.get(\"__UNK\",0))\n",
        "            else:\n",
        "              quadrigram=self.quadrigram.get(tuple(context),self.quadrigram.get(\"__UNK\",{}))\n",
        "              if token in self.wordsim_model.wv.key_to_index:\n",
        "                similar_words = self.wordsim_model.wv.most_similar(token)\n",
        "                for word,prob in self.wordsim_model.wv.most_similar(token):\n",
        "                  small_p=quadrigram.get(word,quadrigram.get(\"__UNK\",0))*(prob/sum([a_tuple[1] for a_tuple in similar_words]))\n",
        "                  big_p+=small_p\n",
        "              else:\n",
        "                big_p = quadrigram.get(token,quadrigram.get(\"__UNK\",0))\n",
        "            return big_p\n",
        "\n",
        "        else:\n",
        "          if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
        "            return lmbda*lmbda*lmbda*self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "          else:\n",
        "            big_p = 0\n",
        "            if methodparams.get(\"method\",\"bigram\")==\"bigram\":\n",
        "              bigram=self.bigram.get(context[0],self.bigram.get(\"__UNK\",{}))\n",
        "              if token in self.wordsim_model.wv.key_to_index:\n",
        "                for word,prob in self.wordsim_model.wv.most_similar(token):\n",
        "                  small_p=bigram.get(word,bigram.get(\"__UNK\",0))*(prob/sum([a_tuple[1] for a_tuple in lm.wordsim_model.wv.most_similar(token)]))\n",
        "                  big_p+=small_p\n",
        "              if big_p == 0:\n",
        "                  big_p = self.get_prob(token=token,context=context,methodparams={\"method\":\"unigram\"},smoothing=\"both\",lmbda=lmbda)\n",
        "                  return big_p\n",
        "              else:\n",
        "                  return big_p*lmbda*lmbda\n",
        "            elif methodparams.get(\"method\",\"trigram\")==\"trigram\":\n",
        "              trigram=self.trigram.get(tuple(context),self.trigram.get(\"__UNK\",{}))\n",
        "              if token in self.wordsim_model.wv.key_to_index:\n",
        "                for word,prob in self.wordsim_model.wv.most_similar(token):\n",
        "                  small_p=trigram.get(word,trigram.get(\"__UNK\",0))*(prob/sum([a_tuple[1] for a_tuple in lm.wordsim_model.wv.most_similar(token)]))\n",
        "                  big_p+=small_p\n",
        "              if big_p==0:\n",
        "                big_p = self.get_prob(token=token,context=context,methodparams={\"method\":\"bigram\"},smoothing=\"both\",lmbda=lmbda)\n",
        "                return big_p\n",
        "              else:\n",
        "                return big_p*lmbda\n",
        "            else:\n",
        "              quadrigram=self.quadrigram.get(tuple(context),self.quadrigram.get(\"__UNK\",{}))\n",
        "              if token in self.wordsim_model.wv.key_to_index:\n",
        "                for word,prob in self.wordsim_model.wv.most_similar(token):\n",
        "                  small_p=quadrigram.get(word,quadrigram.get(\"__UNK\",0))*(prob/sum([a_tuple[1] for a_tuple in lm.wordsim_model.wv.most_similar(token)]))\n",
        "                  big_p+=small_p\n",
        "              if big_p == 0:\n",
        "                big_p = self.get_prob(token=token,context=context,methodparams={\"method\":\"trigram\"},smoothing=\"both\",lmbda=lmbda)\n",
        "              return big_p      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNiPFgnIzU31"
      },
      "outputs": [],
      "source": [
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"both\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"both\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"both\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"both\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Scores"
      ],
      "metadata": {
        "id": "6LLUjLQQGSfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=100 and lambda=0.5"
      ],
      "metadata": {
        "id": "npBRxQXSHacx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_kJ4qYSIL6J"
      },
      "outputs": [],
      "source": [
        "lm = language_model(trainingdir=trainingdir,files=training)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Both smoothing techniques\n",
        "SCC_test = scc_reader()\n",
        "bo_100 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  bo_100.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(bo_100)/len(bo_100))"
      ],
      "metadata": {
        "id": "u_5RQ_VAqBgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=500 and lambda=0.5"
      ],
      "metadata": {
        "id": "kfXC4UqWBk0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class language_model(language_model):\n",
        "  def _make_unknowns(self,known=500):\n",
        "        unknown=0\n",
        "        self.number_unknowns=0\n",
        "        for (k,v) in list(self.unigram.items()):\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "                self.number_unknowns+=1\n",
        "\n",
        "        for (k,adict) in list(self.bigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "\n",
        "        for (k,adict) in list(self.trigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            for word in k:\n",
        "              isknown=self.unigram.get(word,0)\n",
        "              if isknown==0:\n",
        "                del self.trigram[k]\n",
        "                k = list(k)\n",
        "                k[k.index(word)] = \"__UNK\"\n",
        "                k = tuple(k)\n",
        "                current=self.trigram.get(k,{})\n",
        "                current.update(adict)\n",
        "                self.trigram[k]=current\n",
        "              else:\n",
        "                self.trigram[k]=adict"
      ],
      "metadata": {
        "id": "PMKuLNGBqiyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = language_model(trainingdir=trainingdir,files=training)"
      ],
      "metadata": {
        "id": "eORMaNarr_GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Both smoothing techniques\n",
        "SCC_test = scc_reader()\n",
        "bo_500 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  bo_500.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(bo_500)/len(bo_500))"
      ],
      "metadata": {
        "id": "lJEQYezVqyfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=1000 and lambda=0.5"
      ],
      "metadata": {
        "id": "jQ551wwbBvlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class language_model(language_model):\n",
        "  def _make_unknowns(self,known=1000):\n",
        "        unknown=0\n",
        "        self.number_unknowns=0\n",
        "        for (k,v) in list(self.unigram.items()):\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "                self.number_unknowns+=1\n",
        "\n",
        "        for (k,adict) in list(self.bigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "\n",
        "        for (k,adict) in list(self.trigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            for word in k:\n",
        "              isknown=self.unigram.get(word,0)\n",
        "              if isknown==0:\n",
        "                del self.trigram[k]\n",
        "                k = list(k)\n",
        "                k[k.index(word)] = \"__UNK\"\n",
        "                k = tuple(k)\n",
        "                current=self.trigram.get(k,{})\n",
        "                current.update(adict)\n",
        "                self.trigram[k]=current\n",
        "              else:\n",
        "                self.trigram[k]=adict"
      ],
      "metadata": {
        "id": "bCfIU8oFq2eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = language_model(trainingdir=trainingdir,files=training)"
      ],
      "metadata": {
        "id": "h4fnAkqtsAi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Both smoothing techniques\n",
        "SCC_test = scc_reader()\n",
        "bo_1000 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  bo_1000.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(bo_1000)/len(bo_1000))"
      ],
      "metadata": {
        "id": "z_IvP2EEq4_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataset\n",
        "np.random.seed(10)\n",
        "data = [bo_100 , bo_500, bo_1000]\n",
        " \n",
        "fig = plt.figure(figsize =(10, 7))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "ax.set_xticklabels(['100 known', '500 known','1000 known'])\n",
        " \n",
        "# Creating plot\n",
        "plt.boxplot(data)\n",
        "plt.title(\"Box plot for lambda=0.5 and known\") \n",
        "# show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "Yrow26Yim2Te",
        "outputId": "2954cc39-eee1-48a1-bc43-415a58be9302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\benat\\AppData\\Local\\Temp/ipykernel_21152/113886431.py:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(['100 known', '500 known','1000 known'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Distributional smoothing techniques with known=100"
      ],
      "metadata": {
        "id": "-mJVvA24CEA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class language_model(language_model):\n",
        "  def _make_unknowns(self,known=100):\n",
        "        unknown=0\n",
        "        self.number_unknowns=0\n",
        "        for (k,v) in list(self.unigram.items()):\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "                self.number_unknowns+=1\n",
        "\n",
        "        for (k,adict) in list(self.bigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "\n",
        "        for (k,adict) in list(self.trigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            for word in k:\n",
        "              isknown=self.unigram.get(word,0)\n",
        "              if isknown==0:\n",
        "                del self.trigram[k]\n",
        "                k = list(k)\n",
        "                k[k.index(word)] = \"__UNK\"\n",
        "                k = tuple(k)\n",
        "                current=self.trigram.get(k,{})\n",
        "                current.update(adict)\n",
        "                self.trigram[k]=current\n",
        "              else:\n",
        "                self.trigram[k]=adict"
      ],
      "metadata": {
        "id": "tJamAR_YrJcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"distributional\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"distributional\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"distributional\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"distributional\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ],
      "metadata": {
        "id": "O21WOyF2H8wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = language_model(trainingdir=trainingdir,files=training)"
      ],
      "metadata": {
        "id": "1_H3qA8lsG92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distributional smoothing bigram\n",
        "SCC_test = scc_reader()\n",
        "distri_bi = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"bigram\",lm=lm)\n",
        "  print(acc)\n",
        "  distri_bi.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(distri_bi)/len(distri_bi))"
      ],
      "metadata": {
        "id": "1utSArqS9Pv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distributional smoothing trigram\n",
        "SCC_test = scc_reader()\n",
        "distri_tri = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"trigram\",lm=lm)\n",
        "  print(acc)\n",
        "  distri_tri.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(distri_tri)/len(distri_tri))"
      ],
      "metadata": {
        "id": "2e3u2b0qfGxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distributional smoothing quadrigram\n",
        "SCC_test = scc_reader()\n",
        "distri_quad = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  distri_quad.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(distri_quad)/len(distri_quad))"
      ],
      "metadata": {
        "id": "6vwh46i3m-d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Back-Off smoothing techniques with known=100"
      ],
      "metadata": {
        "id": "MHOTy0GQDkDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"back_off\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"back_off\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"back_off\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"back_off\",lmbda=0.5) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ],
      "metadata": {
        "id": "pxVefv1BtCn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Back Off smoothing bigram\n",
        "SCC_test = scc_reader()\n",
        "back_bi = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"bigram\",lm=lm)\n",
        "  print(acc)\n",
        "  back_bi.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(back_bi)/len(back_bi))"
      ],
      "metadata": {
        "id": "Cci08wkvvyyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Back Off smoothing trigram\n",
        "SCC_test = scc_reader()\n",
        "back_tri = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"trigram\",lm=lm)\n",
        "  print(acc)\n",
        "  back_tri.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(back_tri)/len(back_tri))"
      ],
      "metadata": {
        "id": "3MeOHA4p7f6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Back Off smoothing quadrigram\n",
        "SCC_test = scc_reader()\n",
        "back_quad = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  back_quad.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(back_quad)/len(back_quad))"
      ],
      "metadata": {
        "id": "lAa8AxH66yAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=100 and lambda=0.2"
      ],
      "metadata": {
        "id": "weWz9tAEDxwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ],
      "metadata": {
        "id": "P0r6GdSxtSQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "quadri_average_0_02 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  quadri_average_0_02.append(acc)\n",
        "\n",
        "print(\"Score for quadrigram\",sum(quadri_average_0_02)/len(quadri_average_0_02))"
      ],
      "metadata": {
        "id": "_tKgIr1EZXL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=100 and lambda=1"
      ],
      "metadata": {
        "id": "y-q2wc8sE6Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ],
      "metadata": {
        "id": "eT3ezAeOtw6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "quadri_average_0_1 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  quadri_average_0_1.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(quadri_average_0_1)/len(quadri_average_0_1))"
      ],
      "metadata": {
        "id": "gosRyUwuG2LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=500 and lambda=0.2"
      ],
      "metadata": {
        "id": "zU_v47naHmFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class language_model(language_model):\n",
        "  def _make_unknowns(self,known=500):\n",
        "        unknown=0\n",
        "        self.number_unknowns=0\n",
        "        for (k,v) in list(self.unigram.items()):\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "                self.number_unknowns+=1\n",
        "\n",
        "        for (k,adict) in list(self.bigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "\n",
        "        for (k,adict) in list(self.trigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            for word in k:\n",
        "              isknown=self.unigram.get(word,0)\n",
        "              if isknown==0:\n",
        "                del self.trigram[k]\n",
        "                k = list(k)\n",
        "                k[k.index(word)] = \"__UNK\"\n",
        "                k = tuple(k)\n",
        "                current=self.trigram.get(k,{})\n",
        "                current.update(adict)\n",
        "                self.trigram[k]=current\n",
        "              else:\n",
        "                self.trigram[k]=adict\n",
        "\n",
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ],
      "metadata": {
        "id": "p10eiabft3Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = language_model(trainingdir=trainingdir,files=training)"
      ],
      "metadata": {
        "id": "ueNbL5oB0fdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "quadri_average_200_02 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  quadri_average_200_02.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(quadri_average_200_02)/len(quadri_average_200_02))"
      ],
      "metadata": {
        "id": "CcmXPw390hNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=500 and lambda=1"
      ],
      "metadata": {
        "id": "uPUT8CdJFdN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ],
      "metadata": {
        "id": "2n8UN3iNuBE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "quadri_average_500_1 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  quadri_average_500_1.append(acc)\n",
        "\n",
        "print(\"Score for quadrigram\",sum(quadri_average_500_1)/len(quadri_average_500_1))"
      ],
      "metadata": {
        "id": "bgDOFTqpG9ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=1000 and lambda=0.2"
      ],
      "metadata": {
        "id": "uQ2TJyOoHokL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class language_model(language_model):\n",
        "  def _make_unknowns(self,known=1000):\n",
        "        unknown=0\n",
        "        self.number_unknowns=0\n",
        "        for (k,v) in list(self.unigram.items()):\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "                self.number_unknowns+=1\n",
        "\n",
        "        for (k,adict) in list(self.bigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "\n",
        "        for (k,adict) in list(self.trigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0 and not kk==\"__DISCOUNT\":\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            for word in k:\n",
        "              isknown=self.unigram.get(word,0)\n",
        "              if isknown==0:\n",
        "                del self.trigram[k]\n",
        "                k = list(k)\n",
        "                k[k.index(word)] = \"__UNK\"\n",
        "                k = tuple(k)\n",
        "                current=self.trigram.get(k,{})\n",
        "                current.update(adict)\n",
        "                self.trigram[k]=current\n",
        "              else:\n",
        "                self.trigram[k]=adict\n",
        "\n",
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"both\",lmbda=0.2) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ],
      "metadata": {
        "id": "kEUphBUDuF-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = language_model(trainingdir=trainingdir,files=training)"
      ],
      "metadata": {
        "id": "WSWT22M0GXO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "quadri_average_1000_02 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  quadri_average_1000_02.append(acc)\n",
        "\n",
        "print(\"Score for quadrigram\",sum(quadri_average_1000_02)/len(quadri_average_1000_02))"
      ],
      "metadata": {
        "id": "54MA4m77GZYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both smoothing techniques with known=1000 and lambda=1"
      ],
      "metadata": {
        "id": "uATY4d1FFxD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class question(question):\n",
        "    def chooseunigram(self,lm):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]      \n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),0,methodparams={\"method\":\"unigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "        \n",
        "    def choosebigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=1)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"bigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosetrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=2)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"trigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "\n",
        "    def choosequadrigram(self,lm,choices=[]):\n",
        "        if choices==[]:\n",
        "            choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "        context=self.get_left_context(window=3)\n",
        "        probs=[lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":\"quadrigram\"},smoothing=\"both\",lmbda=1) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "    \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)"
      ],
      "metadata": {
        "id": "k3tQrLyquVBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "quadri_average_1000_1 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"quadrigram\",lm=lm)\n",
        "  print(acc)\n",
        "  quadri_average_1000_1.append(acc)\n",
        "\n",
        "print(\"Score for quadrigram\",sum(quadri_average_1000_1)/len(quadri_average_1000_1))"
      ],
      "metadata": {
        "id": "KNbN83J_HHRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEg7zIml3Snf"
      },
      "source": [
        "### Recurrent Neural Network Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create quadrigram and word embeddings"
      ],
      "metadata": {
        "id": "DSdMHUpBX6Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_quadrigram_set(files,training_dir):\n",
        "  alltokens=[\"__END\"]\n",
        "  start=False\n",
        "  for afile in files:\n",
        "    print(\"Processing {}\".format(afile))\n",
        "    try:\n",
        "      with open(os.path.join(training_dir,afile)) as instream:\n",
        "        for line in instream:\n",
        "          line=line.rstrip()\n",
        "          if len(line)>0 and start:\n",
        "            if \"end of project gutenberg\" not in line.lower() and \"end of the project gutenberg\" not in line.lower():\n",
        "              tokens=[\"__END\",\"__START\"]+tokenize(line)+[\"__END\"]\n",
        "              alltokens+=tokens\n",
        "          if \"*END*\" in line:\n",
        "            start = True\n",
        "      start = False\n",
        "    except UnicodeDecodeError:\n",
        "      print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))\n",
        "  \n",
        "  vocab={}\n",
        "  threshold=20\n",
        "  for token in alltokens:\n",
        "    vocab[token]=vocab.get(token,0)+1\n",
        "  unknowns=0\n",
        "  for key,value in list(vocab.items()):\n",
        "    if value < threshold:\n",
        "      unknowns+=value\n",
        "      vocab.pop(key,None)\n",
        "  vocab[\"__UNK\"]=unknowns\n",
        "\n",
        "  word_to_ix = {word: i for i, word in enumerate(list(vocab.keys()))}\n",
        "  ix_to_word = {i: word for i, word in enumerate(list(vocab.keys()))}\n",
        "\n",
        "  filteredtokens=[]\n",
        "  for token in alltokens:\n",
        "    if token in vocab.keys():\n",
        "      filteredtokens.append(token)\n",
        "    else:\n",
        "      filteredtokens.append(\"__UNK\")\n",
        "  quadrigrams = [([filteredtokens[i], filteredtokens[i + 1], filteredtokens[i + 2]], filteredtokens[i + 3])\n",
        "        for i in range(len(filteredtokens) - 3)]\n",
        "  return quadrigrams, vocab, word_to_ix, ix_to_word\n",
        "\n",
        "QUADRIGRAM, VOCAB, WORD_TO_IX, IX_TO_WORD = create_quadrigram_set(files=training,training_dir=trainingdir)"
      ],
      "metadata": {
        "id": "3VKI06S1sw_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating Neural Language Model class"
      ],
      "metadata": {
        "id": "YExrDrVDYAxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available() \n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\") \n",
        "\n",
        "CONTEXT_SIZE = 3\n",
        "EMBEDDING_DIM = 1000\n",
        "\n",
        "class NGramRecurrentLanguageModeler(nn.Module):\n",
        "  def __init__(self, input_size, embedding_dim,hidden_size, output_size, n_layers):\n",
        "        super(NGramRecurrentLanguageModeler, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.embeddings = nn.Embedding(output_size, embedding_dim)\n",
        "        self.hidden_size=hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=self.hidden_size, bidirectional=True, num_layers=n_layers, batch_first=True)   #, bidirectional=True, num_layers=n_layers, batch_first=True\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(self.hidden_size*2, output_size) #output_shape 3\n",
        "\n",
        "  def forward(self, x):\n",
        "        embeds = self.embeddings(x)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)  \n",
        "        c0 = self.init_hidden(batch_size) \n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        x = x.view(*x.shape[:1], -1, *x.shape[3:])\n",
        "        embeds = embeds.view(*embeds.shape[:1], -1, *embeds.shape[3:])\n",
        "        out,_= self.rnn(embeds.unsqueeze(1),(hidden,c0)) #,hidden   embeds.view(len(x)\n",
        "        \n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.reshape(out.shape[0],-1)  #out.contiguous().view(-1, len(VOCAB)) \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out  # , hidden \n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        hidden = torch.zeros(self.n_layers*2, batch_size, self.hidden_size).to(device)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "VIrZ7sG5K-wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training"
      ],
      "metadata": {
        "id": "joz2NeEAf1ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM,hidden_size=128, output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for context, target in QUADRIGRAM:\n",
        "\n",
        "  x_train.append([WORD_TO_IX[w] for w in context])\n",
        "\n",
        "  y_train.append([WORD_TO_IX[target]])      \n",
        "\n",
        "\n",
        "x_train = torch.tensor(x_train,dtype=torch.long).to(device)\n",
        "y_train = torch.tensor(y_train,dtype=torch.long).to(device)\n",
        "\n",
        "  \n",
        "\n",
        "class WordToIxDataset(Dataset):\n",
        "  def __init__(self, X, Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    if len(self.X) != len(self.Y):\n",
        "      raise Exception(\"The length of X does not match the length of Y\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # note that this isn't randomly selecting. It's a simple get a single item that represents an x and y\n",
        "    _x = self.X[index]\n",
        "    _y = self.Y[index]\n",
        "    return _x, _y\n",
        "\n",
        "loader = DataLoader(WordToIxDataset(x_train, y_train), batch_size=8192, shuffle=True, num_workers=0)\n",
        "print('Training set has {} instances'.format(len(loader)))"
      ],
      "metadata": {
        "id": "EbHF8DbmZBj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_iter = iter(loader)\n",
        "imgs, labels = next(train_loader_iter)\n",
        "print(imgs.shape)\n",
        "\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "FImzdR_k_IkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454bdefb-8d45-4ec6-e312-df31df9d12dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8192, 3])\n",
            "torch.Size([8192, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM, hidden_size=128,output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 25\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nLanguageModel.parameters(), lr=lr)\n",
        "\n",
        "losses_001=[]\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  total_loss = 0\n",
        "  for i, (context_x, target_y) in enumerate(loader):\n",
        "    \n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "    output = nLanguageModel(context_x.to(device)) #, hidden\n",
        "    \n",
        "    loss = loss_function(output, target_y.flatten().to(device))\n",
        "    del output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "      tb_x = epoch * len(loader) + i + 1\n",
        "      print('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0. \n",
        "\n",
        "    total_loss += loss.item()\n",
        "  losses_001.append(total_loss)\n",
        "  print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "  print(losses_001)"
      ],
      "metadata": {
        "id": "Sr01vltzeJFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM, hidden_size=128,output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 25\n",
        "lr=0.001\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nLanguageModel.parameters(), lr=lr)\n",
        "\n",
        "losses_0001=[]\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  total_loss = 0\n",
        "  for i, (context_x, target_y) in enumerate(loader):\n",
        "    \n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "    output = nLanguageModel(context_x.to(device)) #, hidden\n",
        "    \n",
        "    loss = loss_function(output, target_y.flatten().to(device))\n",
        "    del output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "      tb_x = epoch * len(loader) + i + 1\n",
        "      print('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0. \n",
        "\n",
        "    total_loss += loss.item()\n",
        "  losses_0001.append(total_loss)\n",
        "  print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "  print(losses_0001)"
      ],
      "metadata": {
        "id": "odSFvM7LJkQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses_001, label='learning rate 0.01')\n",
        "plt.plot(losses_0001, label='learning rate 0.001')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "X8-dTQgL0oQc",
        "outputId": "c7515388-414f-4b50-a744-0f5f0e36975d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Testing"
      ],
      "metadata": {
        "id": "3sFsjnfcf4-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_logprob(context,target):\n",
        "  #return the logprob of the target word given the context\n",
        "  w_in_contxt=[]\n",
        "  for w in context:\n",
        "    if w in WORD_TO_IX.keys():\n",
        "      w_in_contxt.append(WORD_TO_IX[w])\n",
        "    else:\n",
        "      w_in_contxt.append(WORD_TO_IX[\"__UNK\"])\n",
        "  context_idxs = torch.tensor(w_in_contxt, dtype=torch.long)     \n",
        "  context_idxs = context_idxs[None,:]\n",
        "  log_probs= nLanguageModel.forward(context_idxs.to(device))\n",
        "  if target not in WORD_TO_IX.keys():\n",
        "    target_idx=torch.tensor(WORD_TO_IX[\"__UNK\"])\n",
        "  else:\n",
        "    target_idx=torch.tensor(WORD_TO_IX[target])\n",
        "  return log_probs.index_select(1,target_idx.to(device)).item()\n",
        "\n",
        "class question(question):\n",
        "    def choosenmodel(self):\n",
        "        choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]  \n",
        "        context=self.get_left_context(window=3)    \n",
        "        probs=[get_logprob(context=context,target=self.get_field(ch+\")\")) for ch in choices]\n",
        "        maxprob=max(probs)\n",
        "        bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "        return np.random.choice(bestchoices)\n",
        "        \n",
        "    def predict(self,lm,method):\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        elif method==\"random\":\n",
        "            return self.chooserandom()\n",
        "        elif method==\"unigram\":\n",
        "            return self.chooseunigram(lm=lm)\n",
        "        elif method==\"bigram\":\n",
        "            return self.choosebigram(lm=lm)\n",
        "        elif method==\"trigram\":\n",
        "            return self.choosetrigram(lm=lm)\n",
        "        elif method==\"quadrigram\":\n",
        "          return self.choosequadrigram(lm=lm)\n",
        "        elif method==\"nmodel\":\n",
        "          return self.choosenmodel()"
      ],
      "metadata": {
        "id": "ZtbPc2JZgSWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM, hidden_size=32,output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 25\n",
        "lr=0.001\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nLanguageModel.parameters(), lr=lr)\n",
        "\n",
        "losses_0001=[]\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  total_loss = 0\n",
        "  for i, (context_x, target_y) in enumerate(loader):\n",
        "    \n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "    output = nLanguageModel(context_x.to(device)) #, hidden\n",
        "    \n",
        "    loss = loss_function(output, target_y.flatten().to(device))\n",
        "    del output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "      tb_x = epoch * len(loader) + i + 1\n",
        "      print('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0. \n",
        "\n",
        "    total_loss += loss.item()\n",
        "  losses_0001.append(total_loss)\n",
        "  print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "  print(losses_0001)"
      ],
      "metadata": {
        "id": "VmYuYUn-K06E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "lm=None\n",
        "\n",
        "nn_0001_32 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"nmodel\",lm=lm)\n",
        "  print(acc)\n",
        "  nn_0001_32.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(nn_0001_32)/len(nn_0001_32))"
      ],
      "metadata": {
        "id": "VXdzsR3563iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM, hidden_size=32,output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 25\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nLanguageModel.parameters(), lr=lr)\n",
        "\n",
        "losses_0001=[]\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  total_loss = 0\n",
        "  for i, (context_x, target_y) in enumerate(loader):\n",
        "    \n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "    output = nLanguageModel(context_x.to(device)) #, hidden\n",
        "    \n",
        "    loss = loss_function(output, target_y.flatten().to(device))\n",
        "    del output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "      tb_x = epoch * len(loader) + i + 1\n",
        "      print('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0. \n",
        "\n",
        "    total_loss += loss.item()\n",
        "  losses_0001.append(total_loss)\n",
        "  print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "  print(losses_0001)"
      ],
      "metadata": {
        "id": "KNM1xGiGK5oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "lm=None\n",
        "\n",
        "nn_001_32 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"nmodel\",lm=lm)\n",
        "  print(acc)\n",
        "  nn_001_32.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(nn_001_32)/len(nn_001_32))"
      ],
      "metadata": {
        "id": "bqLh5HtG7W79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM, hidden_size=128,output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 25\n",
        "lr=0.001\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nLanguageModel.parameters(), lr=lr)\n",
        "\n",
        "losses_0001=[]\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  total_loss = 0\n",
        "  for i, (context_x, target_y) in enumerate(loader):\n",
        "    \n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "    output = nLanguageModel(context_x.to(device)) #, hidden\n",
        "    \n",
        "    loss = loss_function(output, target_y.flatten().to(device))\n",
        "    del output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "      tb_x = epoch * len(loader) + i + 1\n",
        "      print('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0. \n",
        "\n",
        "    total_loss += loss.item()\n",
        "  losses_0001.append(total_loss)\n",
        "  print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "  print(losses_0001)"
      ],
      "metadata": {
        "id": "_q5pw7weLCVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "lm=None\n",
        "\n",
        "nn_0001_128 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"nmodel\",lm=lm)\n",
        "  print(acc)\n",
        "  nn_0001_128.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(nn_0001_128)/len(nn_0001_128))"
      ],
      "metadata": {
        "id": "9fiC2LfsER7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM, hidden_size=128,output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 25\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nLanguageModel.parameters(), lr=lr)\n",
        "\n",
        "losses_0001=[]\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  total_loss = 0\n",
        "  for i, (context_x, target_y) in enumerate(loader):\n",
        "    \n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "    output = nLanguageModel(context_x.to(device)) #, hidden\n",
        "    \n",
        "    loss = loss_function(output, target_y.flatten().to(device))\n",
        "    del output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "      tb_x = epoch * len(loader) + i + 1\n",
        "      print('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0. \n",
        "\n",
        "    total_loss += loss.item()\n",
        "  losses_0001.append(total_loss)\n",
        "  print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "  print(losses_0001)"
      ],
      "metadata": {
        "id": "_IJBbZTELJ-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "lm=None\n",
        "\n",
        "nn_001_128 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"nmodel\",lm=lm)\n",
        "  print(acc)\n",
        "  nn_001_128.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(nn_001_128)/len(nn_001_128))"
      ],
      "metadata": {
        "id": "TEw4OFEcCu1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM, hidden_size=256,output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 25\n",
        "lr=0.001\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nLanguageModel.parameters(), lr=lr)\n",
        "\n",
        "losses_0001=[]\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  total_loss = 0\n",
        "  for i, (context_x, target_y) in enumerate(loader):\n",
        "    \n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "    output = nLanguageModel(context_x.to(device)) #, hidden\n",
        "    \n",
        "    loss = loss_function(output, target_y.flatten().to(device))\n",
        "    del output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "      tb_x = epoch * len(loader) + i + 1\n",
        "      print('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0. \n",
        "\n",
        "    total_loss += loss.item()\n",
        "  losses_0001.append(total_loss)\n",
        "  print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "  print(losses_0001)"
      ],
      "metadata": {
        "id": "zrSc4C_BLQwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "lm=None\n",
        "\n",
        "nn_0001_256 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"nmodel\",lm=lm)\n",
        "  print(acc)\n",
        "  nn_0001_256.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(nn_0001_256)/len(nn_0001_256))"
      ],
      "metadata": {
        "id": "2tD0H_B_KpW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import dtype\n",
        "# Instantiate the model with hyperparameters\n",
        "nLanguageModel = NGramRecurrentLanguageModeler(input_size=CONTEXT_SIZE*EMBEDDING_DIM, embedding_dim=EMBEDDING_DIM, hidden_size=256,output_size=len(VOCAB), n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "nLanguageModel.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 25\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nLanguageModel.parameters(), lr=lr)\n",
        "\n",
        "losses_001=[]\n",
        "running_loss = 0.\n",
        "last_loss = 0.\n",
        "\n",
        "# Training Run\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  total_loss = 0\n",
        "  for i, (context_x, target_y) in enumerate(loader):\n",
        "    \n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "\n",
        "    output = nLanguageModel(context_x.to(device)) #, hidden\n",
        "    \n",
        "    loss = loss_function(output, target_y.flatten().to(device))\n",
        "    del output\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 1000 == 999:\n",
        "      last_loss = running_loss / 1000 # loss per batch\n",
        "      print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "      tb_x = epoch * len(loader) + i + 1\n",
        "      print('Loss/train', last_loss, tb_x)\n",
        "      running_loss = 0. \n",
        "\n",
        "    total_loss += loss.item()\n",
        "  losses_001.append(total_loss)\n",
        "  print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "  print(losses_001)"
      ],
      "metadata": {
        "id": "3YpS20zuLe9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCC_test = scc_reader()\n",
        "lm=None\n",
        "\n",
        "nn_001_256 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"nmodel\",lm=lm)\n",
        "  print(acc)\n",
        "  nn_001_256.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(nn_001_256)/len(nn_001_256))"
      ],
      "metadata": {
        "id": "pwvVqjIvOhUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Failure cases"
      ],
      "metadata": {
        "id": "q9GCt1yWKuud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCC = scc_reader()\n",
        "SCC.predict_and_score(method=\"quadrigram\",lm=lm)"
      ],
      "metadata": {
        "id": "xU4jQ_4EBVjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nLanguageModel.to(device)\n",
        "SCC_test = scc_reader()\n",
        "lm=None\n",
        "\n",
        "n0n_001_4 = []\n",
        "for i in range(10):\n",
        "  acc = SCC_test.predict_and_score(method=\"nmodel\",lm=lm)\n",
        "  print(acc)\n",
        "  n0n_001_4.append(acc)\n",
        "\n",
        "\n",
        "print(\"Score for quadrigram\",sum(n0n_001_4)/len(n0n_001_4))"
      ],
      "metadata": {
        "id": "7lgYvVxfiA1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_colwidth = 500\n",
        "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
        "answers=os.path.join(parentdir,\"test_answer.csv\")\n",
        "\n",
        "#Visualise the sentences and their possibilities \n",
        "with open(questions) as instream:\n",
        "    csvreader=csv.reader(instream)\n",
        "    lines=list(csvreader)\n",
        "qs_model_df=pd.DataFrame(lines[1:],columns=lines[0])\n",
        "\n",
        "#Visualise the answers to complete the sentence\n",
        "with open(answers) as instream:\n",
        "    csvreader=csv.reader(instream)\n",
        "    lines=list(csvreader)\n",
        "\n",
        "\n",
        "qs_model_df[\"answers\"] = lines[1:]\n",
        "qs_model_df[\"quadrigram_pred\"]=SCC.predict(method=\"quadrigram\",lm=lm)\n",
        "qs_df[\"RNN_pred\"]=SCC.predict(method=\"nmodel\",lm=None) \n",
        "qs_model_df"
      ],
      "metadata": {
        "id": "UcmAUc6nKwnP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "ba8dfa06-69f4-42f6-ac11-d93ac72357cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  \\\n",
              "0        1   \n",
              "1        2   \n",
              "2        3   \n",
              "3        4   \n",
              "4        5   \n",
              "...    ...   \n",
              "1035  1036   \n",
              "1036  1037   \n",
              "1037  1038   \n",
              "1038  1039   \n",
              "1039  1040   \n",
              "\n",
              "                                                                                                                                        question  \\\n",
              "0                                       I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.   \n",
              "1                           It was furnished partly as a sitting and partly as a bedroom , with flowers arranged _____ in every nook and corner.   \n",
              "2                                        As I descended , my old ally , the _____ , came out of the room and closed the door tightly behind him.   \n",
              "3                                                             We got off , _____ our fare , and the trap rattled back on its way to Leatherhead.   \n",
              "4                                                              He held in his hand a _____ of blue paper , scrawled over with notes and figures.   \n",
              "...                                                                                                                                          ...   \n",
              "1035                         The bedrooms in this _____ are on the ground floor , the sitting-rooms being in the central block of the buildings.   \n",
              "1036                                   Our visitor bore every mark of being an average commonplace British tradesman , obese , _____ , and slow.   \n",
              "1037  The terror of his face lay in his eyes , however , steel gray , and glistening coldly with a malignant , inexorable _____ in their depths.   \n",
              "1038             It is your commonplace , _____ crimes which are really puzzling , just as a commonplace face is the most difficult to identify.   \n",
              "1039         On the last occasion he had _____ that if my friend would only come with me he would be glad to extend his hospitality to him also.   \n",
              "\n",
              "              a)               b)             c)          d)             e)  \\\n",
              "0         crying  instantaneously       residing     matched        walking   \n",
              "1       daintily        privately  inadvertently   miserably    comfortably   \n",
              "2           gods             moon        panther       guard  country-dance   \n",
              "3        rubbing         doubling           paid      naming       carrying   \n",
              "4         supply           parcel           sign       sheet         chorus   \n",
              "...          ...              ...            ...         ...            ...   \n",
              "1035        wing            coach        balcony     kingdom  neighbourhood   \n",
              "1036       blind        energetic       eloquent     pompous   sandy-haired   \n",
              "1037     cruelty        novitiate        justice      broker        success   \n",
              "1038  underlying      featureless    theological  flattering     inevitable   \n",
              "1039    believed           proved     discovered    remarked        dreamed   \n",
              "\n",
              "        answers quadrigram_pred  \n",
              "0        [1, c]               b  \n",
              "1        [2, a]               e  \n",
              "2        [3, d]               b  \n",
              "3        [4, c]               e  \n",
              "4        [5, d]               e  \n",
              "...         ...             ...  \n",
              "1035  [1036, a]               d  \n",
              "1036  [1037, d]               a  \n",
              "1037  [1038, a]               a  \n",
              "1038  [1039, b]               c  \n",
              "1039  [1040, d]               e  \n",
              "\n",
              "[1040 rows x 9 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>a)</th>\n",
              "      <th>b)</th>\n",
              "      <th>c)</th>\n",
              "      <th>d)</th>\n",
              "      <th>e)</th>\n",
              "      <th>answers</th>\n",
              "      <th>quadrigram_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I have it from the same source that you are both an orphan and a bachelor and are _____ alone in London.</td>\n",
              "      <td>crying</td>\n",
              "      <td>instantaneously</td>\n",
              "      <td>residing</td>\n",
              "      <td>matched</td>\n",
              "      <td>walking</td>\n",
              "      <td>[1, c]</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>It was furnished partly as a sitting and partly as a bedroom , with flowers arranged _____ in every nook and corner.</td>\n",
              "      <td>daintily</td>\n",
              "      <td>privately</td>\n",
              "      <td>inadvertently</td>\n",
              "      <td>miserably</td>\n",
              "      <td>comfortably</td>\n",
              "      <td>[2, a]</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>As I descended , my old ally , the _____ , came out of the room and closed the door tightly behind him.</td>\n",
              "      <td>gods</td>\n",
              "      <td>moon</td>\n",
              "      <td>panther</td>\n",
              "      <td>guard</td>\n",
              "      <td>country-dance</td>\n",
              "      <td>[3, d]</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>We got off , _____ our fare , and the trap rattled back on its way to Leatherhead.</td>\n",
              "      <td>rubbing</td>\n",
              "      <td>doubling</td>\n",
              "      <td>paid</td>\n",
              "      <td>naming</td>\n",
              "      <td>carrying</td>\n",
              "      <td>[4, c]</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>He held in his hand a _____ of blue paper , scrawled over with notes and figures.</td>\n",
              "      <td>supply</td>\n",
              "      <td>parcel</td>\n",
              "      <td>sign</td>\n",
              "      <td>sheet</td>\n",
              "      <td>chorus</td>\n",
              "      <td>[5, d]</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1035</th>\n",
              "      <td>1036</td>\n",
              "      <td>The bedrooms in this _____ are on the ground floor , the sitting-rooms being in the central block of the buildings.</td>\n",
              "      <td>wing</td>\n",
              "      <td>coach</td>\n",
              "      <td>balcony</td>\n",
              "      <td>kingdom</td>\n",
              "      <td>neighbourhood</td>\n",
              "      <td>[1036, a]</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>1037</td>\n",
              "      <td>Our visitor bore every mark of being an average commonplace British tradesman , obese , _____ , and slow.</td>\n",
              "      <td>blind</td>\n",
              "      <td>energetic</td>\n",
              "      <td>eloquent</td>\n",
              "      <td>pompous</td>\n",
              "      <td>sandy-haired</td>\n",
              "      <td>[1037, d]</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1037</th>\n",
              "      <td>1038</td>\n",
              "      <td>The terror of his face lay in his eyes , however , steel gray , and glistening coldly with a malignant , inexorable _____ in their depths.</td>\n",
              "      <td>cruelty</td>\n",
              "      <td>novitiate</td>\n",
              "      <td>justice</td>\n",
              "      <td>broker</td>\n",
              "      <td>success</td>\n",
              "      <td>[1038, a]</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038</th>\n",
              "      <td>1039</td>\n",
              "      <td>It is your commonplace , _____ crimes which are really puzzling , just as a commonplace face is the most difficult to identify.</td>\n",
              "      <td>underlying</td>\n",
              "      <td>featureless</td>\n",
              "      <td>theological</td>\n",
              "      <td>flattering</td>\n",
              "      <td>inevitable</td>\n",
              "      <td>[1039, b]</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1039</th>\n",
              "      <td>1040</td>\n",
              "      <td>On the last occasion he had _____ that if my friend would only come with me he would be glad to extend his hospitality to him also.</td>\n",
              "      <td>believed</td>\n",
              "      <td>proved</td>\n",
              "      <td>discovered</td>\n",
              "      <td>remarked</td>\n",
              "      <td>dreamed</td>\n",
              "      <td>[1040, d]</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1040 rows  9 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SCC = scc_reader()\n",
        "qs_model_df[\"RNN_pred\"]=SCC.predict(method=\"nmodel\",lm=None) \n"
      ],
      "metadata": {
        "id": "0TtJAYlLfkQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs_model_df.loc[51:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1wUEVFNVi2xa",
        "outputId": "cb71c59a-9a29-4d9d-b0f4-ac4a1675d484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id  \\\n",
              "51    52   \n",
              "52    53   \n",
              "53    54   \n",
              "54    55   \n",
              "55    56   \n",
              "56    57   \n",
              "57    58   \n",
              "58    59   \n",
              "59    60   \n",
              "60    61   \n",
              "61    62   \n",
              "62    63   \n",
              "63    64   \n",
              "64    65   \n",
              "65    66   \n",
              "66    67   \n",
              "67    68   \n",
              "68    69   \n",
              "69    70   \n",
              "70    71   \n",
              "71    72   \n",
              "72    73   \n",
              "73    74   \n",
              "74    75   \n",
              "75    76   \n",
              "76    77   \n",
              "77    78   \n",
              "78    79   \n",
              "79    80   \n",
              "80    81   \n",
              "81    82   \n",
              "82    83   \n",
              "83    84   \n",
              "84    85   \n",
              "85    86   \n",
              "86    87   \n",
              "87    88   \n",
              "88    89   \n",
              "89    90   \n",
              "90    91   \n",
              "91    92   \n",
              "92    93   \n",
              "93    94   \n",
              "94    95   \n",
              "95    96   \n",
              "96    97   \n",
              "97    98   \n",
              "98    99   \n",
              "99   100   \n",
              "100  101   \n",
              "\n",
              "                                                                                                                                                                                       question  \\\n",
              "51                                                                                                   It was one of the main arteries which _____ the traffic of the City to the north and west.   \n",
              "52                                            I stooped in some confusion and began to pick up the fruit , understanding for some reason my companion desired me to take the _____ upon myself.   \n",
              "53                                                                                                He had a very dark , _____ face , and a gleam in his eyes that comes back to me in my dreams.   \n",
              "54                                                                                                                         The fat man _____ his eyes round , and then up at the open skylight.   \n",
              "55                                        We passed up the stair , _____ the door , followed on down a passage , and found ourselves in front of the barricade which Miss Hunter had described.   \n",
              "56                                                                                         Holmes was for the moment as _____ as I. His hand closed like a vice upon my wrist in his agitation.   \n",
              "57                                                                                                                                 The chimney is wide , but is _____ up by four large staples.   \n",
              "58                          My companion sat in the front of the trap , his arms folded , his hat _____ down over his eyes , and his chin sunk upon his breast , buried in the deepest thought.   \n",
              "59                                                                                                 He used to make _____ over the cleverness of women , but I have not heard him do it of late.   \n",
              "60                                      I have _____ , therefore , to call upon you and to consult you in reference to the very painful event which has occurred in connection with my wedding.   \n",
              "61                                                         As I passed out through the wicket gate , however , I found my acquaintance of the morning waiting in the _____ upon the other side.   \n",
              "62                                                                                                                                                        A low _____ had fallen upon our ears.   \n",
              "63                                                                                             I realized it as I _____ back and noted how hill after hill showed traces of the ancient people.   \n",
              "64                                                                                                  You must find your own ink , pens , and blotting-paper , but we _____ this table and chair.   \n",
              "65                                                                                                              I hope to _____ that he has gone , for he has brought nothing but trouble here.   \n",
              "66                                         It threw a livid , unnatural circle upon the floor , while in the _____ beyond we saw the vague loom of two figures which crouched against the wall.   \n",
              "67                                                                                                                              Perhaps our _____ now may do something to make it less obscure.   \n",
              "68                                                                                                                                           That was bad enough , for all that the _____ said.   \n",
              "69                                                                                                                            He had heard nothing , and the _____ remained a complete mystery.   \n",
              "70                                                                                                                                                                 Then he _____ over the hill.   \n",
              "71                                                                                                            Not a _____ , not a rustle , rose now from the dark figure over which we stooped.   \n",
              "72                                                                            I mean to teach them in these parts that law is law , and that there is a man here who does not fear to _____ it.   \n",
              "73                                                                                                           I sat down upon a _____ in the corner and thought the whole matter carefully over.   \n",
              "74                                                          The inspector hurried away on the instant to make _____ about the page , while Holmes and I returned to Baker Street for breakfast.   \n",
              "75                                                She could trust her own guardianship , but she could not tell what _____ or political influence might be brought to bear upon a business man.   \n",
              "76                               When his body had been carried from the cellar we found ourselves still confronted with a problem which was almost as _____ as that with which we had started.   \n",
              "77                                                                                  The back door was open , and as he came to the foot of the _____ he saw two men wrestling together outside.   \n",
              "78                                                                                                                               I read nothing except the criminal _____ and the agony column.   \n",
              "79                                                                                   I confess that they quite _____ my expectations , and that I am utterly unable to account for your result.   \n",
              "80                                                                                                                                                       Was there a police-station _____ near.   \n",
              "81                                                                                                                                         I do not think that I have ever seen so _____ a man.   \n",
              "82                                It was furred outside by a thick layer of dust , and damp and worms had eaten through the wood , so that a crop of livid fungi was _____ on the inside of it.   \n",
              "83                                                                                                                     I don't think I ever _____ faster , but the others were there before us.   \n",
              "84                                                                                                                                              I have tried to _____ it from the measurements.   \n",
              "85                                                                                                                                                                I am sorry to have _____ you.   \n",
              "86                                                                                                                         At the end were the _____ of the high dignitaries who had signed it.   \n",
              "87                                                                              My companion noiselessly _____ the shutters , moved the lamp onto the table , and cast his eyes round the room.   \n",
              "88                                                                                                                           The darkness was _____ , but much was still hidden by the shadows.   \n",
              "89                                                                                                    I was pained at the _____ , for I knew how keenly Holmes would feel any slip of the kind.   \n",
              "90   We had come out upon Oxford Street and I had ventured some remark as to this being a roundabout way to Kensington , when my words were _____ by the extraordinary conduct of my companion.   \n",
              "91                     When I thought of the heavy _____ and looked at the gaping roof I understood how strong and immutable must be the purpose which had kept him in that inhospitable abode.   \n",
              "92                              When leaving the house she was heard by the coachman to make some commonplace remark to her husband , and to _____ him that she would be back before very long.   \n",
              "93                                                                                                                                            Was she his _____ , his friend , or his mistress.   \n",
              "94                          We had hardly reached the hall when we heard the _____ of a hound , and then a scream of agony , with a horrible worrying sound which it was dreadful to listen to.   \n",
              "95                                                                 A collection of my trifling achievements would certainly be _____ which contained no account of this very singular business.   \n",
              "96                                                                                   His hair and whiskers were shot with gray , and his face was all crinkled and _____ like a withered apple.   \n",
              "97                     The point is a simple one , but the Inspector had _____ it because he had started with the supposition that these county magnates had had nothing to do with the matter.   \n",
              "98                                                                                                       He went over to the door , and turning the _____ he examined it in his methodical way.   \n",
              "99                                                                                                                                                        My heart _____ within me as I saw it.   \n",
              "100                                          Another day two at the most and I have my case _____ , but until then guard your charge as closely as ever a fond mother watched her ailing child.   \n",
              "\n",
              "               a)            b)          c)           d)             e)  \\\n",
              "51      supported    surrounded   sheltered     conveyed      contained   \n",
              "52           oath       message       blame        shirt        shadows   \n",
              "53           rosy      childish    fearsome    colorless         yellow   \n",
              "54           cast        folded    hastened      jingled         winked   \n",
              "55       devoured    translated    unlocked     ascended       occupied   \n",
              "56       startled         tired        thin         wise         clever   \n",
              "57           torn      softened      higher       barred     constantly   \n",
              "58         closed      slouched     wheeled       pulled          blown   \n",
              "59       progress       matters       merry     advances          rules   \n",
              "60     determined       related   overheard    absconded        noticed   \n",
              "61         battle       chamber   incubator       shadow         spring   \n",
              "62      courtship      struggle        blow        shrub           moan   \n",
              "63         sailed        smiled       drove       shrunk         flowed   \n",
              "64         behold      realised     provide         lose            owe   \n",
              "65         heaven       explain       argue      inhabit         demand   \n",
              "66        shadows       thicket      rushes       potage          field   \n",
              "67           gate       weather       visit        limbs         skirts   \n",
              "68         garret        saints         dog      coroner          birds   \n",
              "69         affair         devil        snow    challenge       illusion   \n",
              "70       vanished          bent       leant         wept        hovered   \n",
              "71      scoundrel        muddle     soldier         whim        whisper   \n",
              "72           pawn    contradict      invoke        store         subdue   \n",
              "73            keg         sword        farm          fly         needle   \n",
              "74          songs  pleasantries    mistakes      stories      inquiries   \n",
              "75          sport      indirect    handsome   imprudence        comfort   \n",
              "76     formidable          loud       quick         tall      invisible   \n",
              "77         forest       parties     victory       stairs          mills   \n",
              "78        courage         peace        news         nuts          plans   \n",
              "79        visited       enjoyed   lightened      emptied        surpass   \n",
              "80          lying       rolling    gathered     anywhere         comin'   \n",
              "81       accurate      numerous        thin  provocative         marked   \n",
              "82     resounding      sleeping    beheaded      running        growing   \n",
              "83         choked     suspected       drove   remembered      conceived   \n",
              "84      recommend         fling  accomplish  reconstruct          carry   \n",
              "85         killed     convinced   practised     expected    interrupted   \n",
              "86     signatures        relics      hearts    anxieties      portraits   \n",
              "87           toed       awaited     grasped       closed        mounted   \n",
              "88         rising        healed   ponderous    neglected     attractive   \n",
              "89        mistake         porch   fireplace      ceiling           pump   \n",
              "90       arrested      startled    enriched    perplexed       benumbed   \n",
              "91           step    sandwiches   breathing        rains          boxes   \n",
              "92         assure        detach       teach      dismiss       reproach   \n",
              "93   discomfiture        client      choice      musings    opportunity   \n",
              "94          image       clatter      baying       tinkle          click   \n",
              "95     incomplete    considered     audible   discovered  disembowelled   \n",
              "96     chattering   picturesque     hopeful     puckered     glistening   \n",
              "97       detected         baked     touched     rendered     overlooked   \n",
              "98           lamp          fish        lock       bucket        boulder   \n",
              "99        cruised          lies    lingered       leaped      struggled   \n",
              "100       moaning         arose    complete      tonight   mechanically   \n",
              "\n",
              "      answers quadrigram_pred RNN_pred  \n",
              "51    [52, d]               e        b  \n",
              "52    [53, c]               b        e  \n",
              "53    [54, c]               d        e  \n",
              "54    [55, a]               e        c  \n",
              "55    [56, c]               b        a  \n",
              "56    [57, a]               d        c  \n",
              "57    [58, d]               a        a  \n",
              "58    [59, d]               d        e  \n",
              "59    [60, c]               c        e  \n",
              "60    [61, a]               e        d  \n",
              "61    [62, d]               b        a  \n",
              "62    [63, e]               e        b  \n",
              "63    [64, c]               c        b  \n",
              "64    [65, c]               b        e  \n",
              "65    [66, a]               a        a  \n",
              "66    [67, a]               e        c  \n",
              "67    [68, c]               a        d  \n",
              "68    [69, d]               e        c  \n",
              "69    [70, a]               c        b  \n",
              "70    [71, a]               c        a  \n",
              "71    [72, e]               c        a  \n",
              "72    [73, c]               b        d  \n",
              "73    [74, a]               c        e  \n",
              "74    [75, e]               a        b  \n",
              "75    [76, b]               c        e  \n",
              "76    [77, a]               c        c  \n",
              "77    [78, d]               d        e  \n",
              "78    [79, c]               d        b  \n",
              "79    [80, e]               a        e  \n",
              "80    [81, d]               d        a  \n",
              "81    [82, c]               e        c  \n",
              "82    [83, e]               c        d  \n",
              "83    [84, c]               d        d  \n",
              "84    [85, d]               e        e  \n",
              "85    [86, e]               e        d  \n",
              "86    [87, a]               c        e  \n",
              "87    [88, d]               d        d  \n",
              "88    [89, a]               e        c  \n",
              "89    [90, a]               b        a  \n",
              "90    [91, a]               d        b  \n",
              "91    [92, d]               d        e  \n",
              "92    [93, a]               c        e  \n",
              "93    [94, b]               c        e  \n",
              "94    [95, c]               b        a  \n",
              "95    [96, a]               d        d  \n",
              "96    [97, d]               c        d  \n",
              "97    [98, e]               b        e  \n",
              "98    [99, c]               c        d  \n",
              "99   [100, d]               e        b  \n",
              "100  [101, c]               d        c  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>a)</th>\n",
              "      <th>b)</th>\n",
              "      <th>c)</th>\n",
              "      <th>d)</th>\n",
              "      <th>e)</th>\n",
              "      <th>answers</th>\n",
              "      <th>quadrigram_pred</th>\n",
              "      <th>RNN_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>52</td>\n",
              "      <td>It was one of the main arteries which _____ the traffic of the City to the north and west.</td>\n",
              "      <td>supported</td>\n",
              "      <td>surrounded</td>\n",
              "      <td>sheltered</td>\n",
              "      <td>conveyed</td>\n",
              "      <td>contained</td>\n",
              "      <td>[52, d]</td>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>53</td>\n",
              "      <td>I stooped in some confusion and began to pick up the fruit , understanding for some reason my companion desired me to take the _____ upon myself.</td>\n",
              "      <td>oath</td>\n",
              "      <td>message</td>\n",
              "      <td>blame</td>\n",
              "      <td>shirt</td>\n",
              "      <td>shadows</td>\n",
              "      <td>[53, c]</td>\n",
              "      <td>b</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>54</td>\n",
              "      <td>He had a very dark , _____ face , and a gleam in his eyes that comes back to me in my dreams.</td>\n",
              "      <td>rosy</td>\n",
              "      <td>childish</td>\n",
              "      <td>fearsome</td>\n",
              "      <td>colorless</td>\n",
              "      <td>yellow</td>\n",
              "      <td>[54, c]</td>\n",
              "      <td>d</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>55</td>\n",
              "      <td>The fat man _____ his eyes round , and then up at the open skylight.</td>\n",
              "      <td>cast</td>\n",
              "      <td>folded</td>\n",
              "      <td>hastened</td>\n",
              "      <td>jingled</td>\n",
              "      <td>winked</td>\n",
              "      <td>[55, a]</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>56</td>\n",
              "      <td>We passed up the stair , _____ the door , followed on down a passage , and found ourselves in front of the barricade which Miss Hunter had described.</td>\n",
              "      <td>devoured</td>\n",
              "      <td>translated</td>\n",
              "      <td>unlocked</td>\n",
              "      <td>ascended</td>\n",
              "      <td>occupied</td>\n",
              "      <td>[56, c]</td>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>57</td>\n",
              "      <td>Holmes was for the moment as _____ as I. His hand closed like a vice upon my wrist in his agitation.</td>\n",
              "      <td>startled</td>\n",
              "      <td>tired</td>\n",
              "      <td>thin</td>\n",
              "      <td>wise</td>\n",
              "      <td>clever</td>\n",
              "      <td>[57, a]</td>\n",
              "      <td>d</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>58</td>\n",
              "      <td>The chimney is wide , but is _____ up by four large staples.</td>\n",
              "      <td>torn</td>\n",
              "      <td>softened</td>\n",
              "      <td>higher</td>\n",
              "      <td>barred</td>\n",
              "      <td>constantly</td>\n",
              "      <td>[58, d]</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>59</td>\n",
              "      <td>My companion sat in the front of the trap , his arms folded , his hat _____ down over his eyes , and his chin sunk upon his breast , buried in the deepest thought.</td>\n",
              "      <td>closed</td>\n",
              "      <td>slouched</td>\n",
              "      <td>wheeled</td>\n",
              "      <td>pulled</td>\n",
              "      <td>blown</td>\n",
              "      <td>[59, d]</td>\n",
              "      <td>d</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>60</td>\n",
              "      <td>He used to make _____ over the cleverness of women , but I have not heard him do it of late.</td>\n",
              "      <td>progress</td>\n",
              "      <td>matters</td>\n",
              "      <td>merry</td>\n",
              "      <td>advances</td>\n",
              "      <td>rules</td>\n",
              "      <td>[60, c]</td>\n",
              "      <td>c</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>61</td>\n",
              "      <td>I have _____ , therefore , to call upon you and to consult you in reference to the very painful event which has occurred in connection with my wedding.</td>\n",
              "      <td>determined</td>\n",
              "      <td>related</td>\n",
              "      <td>overheard</td>\n",
              "      <td>absconded</td>\n",
              "      <td>noticed</td>\n",
              "      <td>[61, a]</td>\n",
              "      <td>e</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>62</td>\n",
              "      <td>As I passed out through the wicket gate , however , I found my acquaintance of the morning waiting in the _____ upon the other side.</td>\n",
              "      <td>battle</td>\n",
              "      <td>chamber</td>\n",
              "      <td>incubator</td>\n",
              "      <td>shadow</td>\n",
              "      <td>spring</td>\n",
              "      <td>[62, d]</td>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>63</td>\n",
              "      <td>A low _____ had fallen upon our ears.</td>\n",
              "      <td>courtship</td>\n",
              "      <td>struggle</td>\n",
              "      <td>blow</td>\n",
              "      <td>shrub</td>\n",
              "      <td>moan</td>\n",
              "      <td>[63, e]</td>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>64</td>\n",
              "      <td>I realized it as I _____ back and noted how hill after hill showed traces of the ancient people.</td>\n",
              "      <td>sailed</td>\n",
              "      <td>smiled</td>\n",
              "      <td>drove</td>\n",
              "      <td>shrunk</td>\n",
              "      <td>flowed</td>\n",
              "      <td>[64, c]</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>65</td>\n",
              "      <td>You must find your own ink , pens , and blotting-paper , but we _____ this table and chair.</td>\n",
              "      <td>behold</td>\n",
              "      <td>realised</td>\n",
              "      <td>provide</td>\n",
              "      <td>lose</td>\n",
              "      <td>owe</td>\n",
              "      <td>[65, c]</td>\n",
              "      <td>b</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>66</td>\n",
              "      <td>I hope to _____ that he has gone , for he has brought nothing but trouble here.</td>\n",
              "      <td>heaven</td>\n",
              "      <td>explain</td>\n",
              "      <td>argue</td>\n",
              "      <td>inhabit</td>\n",
              "      <td>demand</td>\n",
              "      <td>[66, a]</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>67</td>\n",
              "      <td>It threw a livid , unnatural circle upon the floor , while in the _____ beyond we saw the vague loom of two figures which crouched against the wall.</td>\n",
              "      <td>shadows</td>\n",
              "      <td>thicket</td>\n",
              "      <td>rushes</td>\n",
              "      <td>potage</td>\n",
              "      <td>field</td>\n",
              "      <td>[67, a]</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>68</td>\n",
              "      <td>Perhaps our _____ now may do something to make it less obscure.</td>\n",
              "      <td>gate</td>\n",
              "      <td>weather</td>\n",
              "      <td>visit</td>\n",
              "      <td>limbs</td>\n",
              "      <td>skirts</td>\n",
              "      <td>[68, c]</td>\n",
              "      <td>a</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>69</td>\n",
              "      <td>That was bad enough , for all that the _____ said.</td>\n",
              "      <td>garret</td>\n",
              "      <td>saints</td>\n",
              "      <td>dog</td>\n",
              "      <td>coroner</td>\n",
              "      <td>birds</td>\n",
              "      <td>[69, d]</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>70</td>\n",
              "      <td>He had heard nothing , and the _____ remained a complete mystery.</td>\n",
              "      <td>affair</td>\n",
              "      <td>devil</td>\n",
              "      <td>snow</td>\n",
              "      <td>challenge</td>\n",
              "      <td>illusion</td>\n",
              "      <td>[70, a]</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>71</td>\n",
              "      <td>Then he _____ over the hill.</td>\n",
              "      <td>vanished</td>\n",
              "      <td>bent</td>\n",
              "      <td>leant</td>\n",
              "      <td>wept</td>\n",
              "      <td>hovered</td>\n",
              "      <td>[71, a]</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>72</td>\n",
              "      <td>Not a _____ , not a rustle , rose now from the dark figure over which we stooped.</td>\n",
              "      <td>scoundrel</td>\n",
              "      <td>muddle</td>\n",
              "      <td>soldier</td>\n",
              "      <td>whim</td>\n",
              "      <td>whisper</td>\n",
              "      <td>[72, e]</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>73</td>\n",
              "      <td>I mean to teach them in these parts that law is law , and that there is a man here who does not fear to _____ it.</td>\n",
              "      <td>pawn</td>\n",
              "      <td>contradict</td>\n",
              "      <td>invoke</td>\n",
              "      <td>store</td>\n",
              "      <td>subdue</td>\n",
              "      <td>[73, c]</td>\n",
              "      <td>b</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>74</td>\n",
              "      <td>I sat down upon a _____ in the corner and thought the whole matter carefully over.</td>\n",
              "      <td>keg</td>\n",
              "      <td>sword</td>\n",
              "      <td>farm</td>\n",
              "      <td>fly</td>\n",
              "      <td>needle</td>\n",
              "      <td>[74, a]</td>\n",
              "      <td>c</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>75</td>\n",
              "      <td>The inspector hurried away on the instant to make _____ about the page , while Holmes and I returned to Baker Street for breakfast.</td>\n",
              "      <td>songs</td>\n",
              "      <td>pleasantries</td>\n",
              "      <td>mistakes</td>\n",
              "      <td>stories</td>\n",
              "      <td>inquiries</td>\n",
              "      <td>[75, e]</td>\n",
              "      <td>a</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>76</td>\n",
              "      <td>She could trust her own guardianship , but she could not tell what _____ or political influence might be brought to bear upon a business man.</td>\n",
              "      <td>sport</td>\n",
              "      <td>indirect</td>\n",
              "      <td>handsome</td>\n",
              "      <td>imprudence</td>\n",
              "      <td>comfort</td>\n",
              "      <td>[76, b]</td>\n",
              "      <td>c</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>77</td>\n",
              "      <td>When his body had been carried from the cellar we found ourselves still confronted with a problem which was almost as _____ as that with which we had started.</td>\n",
              "      <td>formidable</td>\n",
              "      <td>loud</td>\n",
              "      <td>quick</td>\n",
              "      <td>tall</td>\n",
              "      <td>invisible</td>\n",
              "      <td>[77, a]</td>\n",
              "      <td>c</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>78</td>\n",
              "      <td>The back door was open , and as he came to the foot of the _____ he saw two men wrestling together outside.</td>\n",
              "      <td>forest</td>\n",
              "      <td>parties</td>\n",
              "      <td>victory</td>\n",
              "      <td>stairs</td>\n",
              "      <td>mills</td>\n",
              "      <td>[78, d]</td>\n",
              "      <td>d</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>79</td>\n",
              "      <td>I read nothing except the criminal _____ and the agony column.</td>\n",
              "      <td>courage</td>\n",
              "      <td>peace</td>\n",
              "      <td>news</td>\n",
              "      <td>nuts</td>\n",
              "      <td>plans</td>\n",
              "      <td>[79, c]</td>\n",
              "      <td>d</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>80</td>\n",
              "      <td>I confess that they quite _____ my expectations , and that I am utterly unable to account for your result.</td>\n",
              "      <td>visited</td>\n",
              "      <td>enjoyed</td>\n",
              "      <td>lightened</td>\n",
              "      <td>emptied</td>\n",
              "      <td>surpass</td>\n",
              "      <td>[80, e]</td>\n",
              "      <td>a</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>81</td>\n",
              "      <td>Was there a police-station _____ near.</td>\n",
              "      <td>lying</td>\n",
              "      <td>rolling</td>\n",
              "      <td>gathered</td>\n",
              "      <td>anywhere</td>\n",
              "      <td>comin'</td>\n",
              "      <td>[81, d]</td>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>82</td>\n",
              "      <td>I do not think that I have ever seen so _____ a man.</td>\n",
              "      <td>accurate</td>\n",
              "      <td>numerous</td>\n",
              "      <td>thin</td>\n",
              "      <td>provocative</td>\n",
              "      <td>marked</td>\n",
              "      <td>[82, c]</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>83</td>\n",
              "      <td>It was furred outside by a thick layer of dust , and damp and worms had eaten through the wood , so that a crop of livid fungi was _____ on the inside of it.</td>\n",
              "      <td>resounding</td>\n",
              "      <td>sleeping</td>\n",
              "      <td>beheaded</td>\n",
              "      <td>running</td>\n",
              "      <td>growing</td>\n",
              "      <td>[83, e]</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>84</td>\n",
              "      <td>I don't think I ever _____ faster , but the others were there before us.</td>\n",
              "      <td>choked</td>\n",
              "      <td>suspected</td>\n",
              "      <td>drove</td>\n",
              "      <td>remembered</td>\n",
              "      <td>conceived</td>\n",
              "      <td>[84, c]</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>85</td>\n",
              "      <td>I have tried to _____ it from the measurements.</td>\n",
              "      <td>recommend</td>\n",
              "      <td>fling</td>\n",
              "      <td>accomplish</td>\n",
              "      <td>reconstruct</td>\n",
              "      <td>carry</td>\n",
              "      <td>[85, d]</td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>86</td>\n",
              "      <td>I am sorry to have _____ you.</td>\n",
              "      <td>killed</td>\n",
              "      <td>convinced</td>\n",
              "      <td>practised</td>\n",
              "      <td>expected</td>\n",
              "      <td>interrupted</td>\n",
              "      <td>[86, e]</td>\n",
              "      <td>e</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>87</td>\n",
              "      <td>At the end were the _____ of the high dignitaries who had signed it.</td>\n",
              "      <td>signatures</td>\n",
              "      <td>relics</td>\n",
              "      <td>hearts</td>\n",
              "      <td>anxieties</td>\n",
              "      <td>portraits</td>\n",
              "      <td>[87, a]</td>\n",
              "      <td>c</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>88</td>\n",
              "      <td>My companion noiselessly _____ the shutters , moved the lamp onto the table , and cast his eyes round the room.</td>\n",
              "      <td>toed</td>\n",
              "      <td>awaited</td>\n",
              "      <td>grasped</td>\n",
              "      <td>closed</td>\n",
              "      <td>mounted</td>\n",
              "      <td>[88, d]</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>89</td>\n",
              "      <td>The darkness was _____ , but much was still hidden by the shadows.</td>\n",
              "      <td>rising</td>\n",
              "      <td>healed</td>\n",
              "      <td>ponderous</td>\n",
              "      <td>neglected</td>\n",
              "      <td>attractive</td>\n",
              "      <td>[89, a]</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>90</td>\n",
              "      <td>I was pained at the _____ , for I knew how keenly Holmes would feel any slip of the kind.</td>\n",
              "      <td>mistake</td>\n",
              "      <td>porch</td>\n",
              "      <td>fireplace</td>\n",
              "      <td>ceiling</td>\n",
              "      <td>pump</td>\n",
              "      <td>[90, a]</td>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>91</td>\n",
              "      <td>We had come out upon Oxford Street and I had ventured some remark as to this being a roundabout way to Kensington , when my words were _____ by the extraordinary conduct of my companion.</td>\n",
              "      <td>arrested</td>\n",
              "      <td>startled</td>\n",
              "      <td>enriched</td>\n",
              "      <td>perplexed</td>\n",
              "      <td>benumbed</td>\n",
              "      <td>[91, a]</td>\n",
              "      <td>d</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>92</td>\n",
              "      <td>When I thought of the heavy _____ and looked at the gaping roof I understood how strong and immutable must be the purpose which had kept him in that inhospitable abode.</td>\n",
              "      <td>step</td>\n",
              "      <td>sandwiches</td>\n",
              "      <td>breathing</td>\n",
              "      <td>rains</td>\n",
              "      <td>boxes</td>\n",
              "      <td>[92, d]</td>\n",
              "      <td>d</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>93</td>\n",
              "      <td>When leaving the house she was heard by the coachman to make some commonplace remark to her husband , and to _____ him that she would be back before very long.</td>\n",
              "      <td>assure</td>\n",
              "      <td>detach</td>\n",
              "      <td>teach</td>\n",
              "      <td>dismiss</td>\n",
              "      <td>reproach</td>\n",
              "      <td>[93, a]</td>\n",
              "      <td>c</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>94</td>\n",
              "      <td>Was she his _____ , his friend , or his mistress.</td>\n",
              "      <td>discomfiture</td>\n",
              "      <td>client</td>\n",
              "      <td>choice</td>\n",
              "      <td>musings</td>\n",
              "      <td>opportunity</td>\n",
              "      <td>[94, b]</td>\n",
              "      <td>c</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>95</td>\n",
              "      <td>We had hardly reached the hall when we heard the _____ of a hound , and then a scream of agony , with a horrible worrying sound which it was dreadful to listen to.</td>\n",
              "      <td>image</td>\n",
              "      <td>clatter</td>\n",
              "      <td>baying</td>\n",
              "      <td>tinkle</td>\n",
              "      <td>click</td>\n",
              "      <td>[95, c]</td>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>96</td>\n",
              "      <td>A collection of my trifling achievements would certainly be _____ which contained no account of this very singular business.</td>\n",
              "      <td>incomplete</td>\n",
              "      <td>considered</td>\n",
              "      <td>audible</td>\n",
              "      <td>discovered</td>\n",
              "      <td>disembowelled</td>\n",
              "      <td>[96, a]</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>97</td>\n",
              "      <td>His hair and whiskers were shot with gray , and his face was all crinkled and _____ like a withered apple.</td>\n",
              "      <td>chattering</td>\n",
              "      <td>picturesque</td>\n",
              "      <td>hopeful</td>\n",
              "      <td>puckered</td>\n",
              "      <td>glistening</td>\n",
              "      <td>[97, d]</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>98</td>\n",
              "      <td>The point is a simple one , but the Inspector had _____ it because he had started with the supposition that these county magnates had had nothing to do with the matter.</td>\n",
              "      <td>detected</td>\n",
              "      <td>baked</td>\n",
              "      <td>touched</td>\n",
              "      <td>rendered</td>\n",
              "      <td>overlooked</td>\n",
              "      <td>[98, e]</td>\n",
              "      <td>b</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>99</td>\n",
              "      <td>He went over to the door , and turning the _____ he examined it in his methodical way.</td>\n",
              "      <td>lamp</td>\n",
              "      <td>fish</td>\n",
              "      <td>lock</td>\n",
              "      <td>bucket</td>\n",
              "      <td>boulder</td>\n",
              "      <td>[99, c]</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>100</td>\n",
              "      <td>My heart _____ within me as I saw it.</td>\n",
              "      <td>cruised</td>\n",
              "      <td>lies</td>\n",
              "      <td>lingered</td>\n",
              "      <td>leaped</td>\n",
              "      <td>struggled</td>\n",
              "      <td>[100, d]</td>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>101</td>\n",
              "      <td>Another day two at the most and I have my case _____ , but until then guard your charge as closely as ever a fond mother watched her ailing child.</td>\n",
              "      <td>moaning</td>\n",
              "      <td>arose</td>\n",
              "      <td>complete</td>\n",
              "      <td>tonight</td>\n",
              "      <td>mechanically</td>\n",
              "      <td>[101, c]</td>\n",
              "      <td>d</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jeDnM6XE8RFg",
        "ysirdkKwzvBt",
        "mKGPutjy_3SF",
        "0ei7fseU_3Fl",
        "JXsFr75a3N9n"
      ],
      "name": "Advanced NLE Assignment Language Models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}